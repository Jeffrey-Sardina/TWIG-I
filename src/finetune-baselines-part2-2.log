loading model settings from cache: checkpoints/FB15k237-pretrain-chkpt-ID_6385341367018417.pkl
Running job! The arguemnts recieved are:
	 dataset_names: WN18RR
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0
)
	 optimizer_args: {'lr': 0.005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 10, 'npp': 500, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: FB15k237-to-WN18RR-from-chkpt-ID_6385341367018417_e10

loading dataset
WN18RR
X_p: torch.Size([86835, 22])
n_local: 21
X_p: torch.Size([2924, 22])
n_local: 21
X_p: torch.Size([2824, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 679 loss: 0.08081480115652084
batch 500 / 679 loss: 0.07343918830156326
Epoch 2 -- batch 0 / 679 loss: 0.07887595146894455
batch 500 / 679 loss: 0.07503949105739594
Epoch 3 -- batch 0 / 679 loss: 0.08173715323209763
batch 500 / 679 loss: 0.0757473036646843
Epoch 4 -- batch 0 / 679 loss: 0.07911181449890137
batch 500 / 679 loss: 0.0739133208990097
Epoch 5 -- batch 0 / 679 loss: 0.08148318529129028
batch 500 / 679 loss: 0.07297201454639435
Saving checkpoint at epoch 5; prefix = FB15k237-to-WN18RR-from-chkpt-ID_6385341367018417_e10-chkpt-ID_2874982758798220
Epoch 6 -- batch 0 / 679 loss: 0.07987537235021591
batch 500 / 679 loss: 0.07184448093175888
Epoch 7 -- batch 0 / 679 loss: 0.07889369130134583
batch 500 / 679 loss: 0.0740308165550232
Epoch 8 -- batch 0 / 679 loss: 0.08066858351230621
batch 500 / 679 loss: 0.0734017938375473
Epoch 9 -- batch 0 / 679 loss: 0.07969959080219269
batch 500 / 679 loss: 0.07420847564935684
Epoch 10 -- batch 0 / 679 loss: 0.07907646894454956
batch 500 / 679 loss: 0.07403713464736938
Saving checkpoint at epoch 10; prefix = FB15k237-to-WN18RR-from-chkpt-ID_6385341367018417_e10-chkpt-ID_2874982758798220
Done Training!

==================================
Testing (cite this): dataloader for dataset WN18RR
Testing (cite this): batch 0 / 46
batch 0 / 46 loss: 0.06049833446741104
Testing (cite this): batch 1 / 46
Testing (cite this): batch 2 / 46
Testing (cite this): batch 3 / 46
Testing (cite this): batch 4 / 46
Testing (cite this): batch 5 / 46
Testing (cite this): batch 6 / 46
Testing (cite this): batch 7 / 46
Testing (cite this): batch 8 / 46
Testing (cite this): batch 9 / 46
Testing (cite this): batch 10 / 46
Testing (cite this): batch 11 / 46
Testing (cite this): batch 12 / 46
Testing (cite this): batch 13 / 46
Testing (cite this): batch 14 / 46
Testing (cite this): batch 15 / 46
Testing (cite this): batch 16 / 46
Testing (cite this): batch 17 / 46
Testing (cite this): batch 18 / 46
Testing (cite this): batch 19 / 46
Testing (cite this): batch 20 / 46
Testing (cite this): batch 21 / 46
Testing (cite this): batch 22 / 46
Testing (cite this): batch 23 / 46
Testing (cite this): batch 24 / 46
Testing (cite this): batch 25 / 46
Testing (cite this): batch 26 / 46
Testing (cite this): batch 27 / 46
Testing (cite this): batch 28 / 46
Testing (cite this): batch 29 / 46
Testing (cite this): batch 30 / 46
Testing (cite this): batch 31 / 46
Testing (cite this): batch 32 / 46
Testing (cite this): batch 33 / 46
Testing (cite this): batch 34 / 46
Testing (cite this): batch 35 / 46
Testing (cite this): batch 36 / 46
Testing (cite this): batch 37 / 46
Testing (cite this): batch 38 / 46
Testing (cite this): batch 39 / 46
Testing (cite this): batch 40 / 46
Testing (cite this): batch 41 / 46
Testing (cite this): batch 42 / 46
Testing (cite this): batch 43 / 46
Testing (cite this): batch 44 / 46
Testing (cite this): batch 45 / 46
total number of ranks, torch.Size([5848])
====== Ranks ======
ranks size: torch.Size([5848])
test_loss: 2.3061693096533418
mr: 9318.1201171875
mrr: 0.27002575993537903
h1: 0.24350205063819885
h3: 0.28231874108314514
h5: 0.297537624835968
h10: 0.31634747982025146
==================================

Done Testing!
done with training and eval
loading model settings from cache: checkpoints/WN18RR-pretrain-chkpt-ID_5360871134295512.pkl
Running job! The arguemnts recieved are:
	 dataset_names: CoDExSmall
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
	 optimizer_args: {'lr': 0.005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 64, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 10, 'npp': 100, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: WN18RR-to-CoDExSmall-from-chkpt-ID_5360871134295512_e10

loading dataset
CoDExSmall
X_p: torch.Size([32888, 22])
n_local: 21
X_p: torch.Size([1828, 22])
n_local: 21
X_p: torch.Size([1827, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 514 loss: 0.0896635502576828
batch 500 / 514 loss: 0.09508085250854492
Epoch 2 -- batch 0 / 514 loss: 0.08941386640071869
batch 500 / 514 loss: 0.09690415859222412
Epoch 3 -- batch 0 / 514 loss: 0.08764530718326569
batch 500 / 514 loss: 0.09394615888595581
Epoch 4 -- batch 0 / 514 loss: 0.08806382119655609
batch 500 / 514 loss: 0.09269480407238007
Epoch 5 -- batch 0 / 514 loss: 0.09005426615476608
batch 500 / 514 loss: 0.09523992240428925
Saving checkpoint at epoch 5; prefix = WN18RR-to-CoDExSmall-from-chkpt-ID_5360871134295512_e10-chkpt-ID_281109503281120
Epoch 6 -- batch 0 / 514 loss: 0.0871562510728836
batch 500 / 514 loss: 0.09532369673252106
Epoch 7 -- batch 0 / 514 loss: 0.08947095274925232
batch 500 / 514 loss: 0.093874491751194
Epoch 8 -- batch 0 / 514 loss: 0.08869312703609467
batch 500 / 514 loss: 0.09451113641262054
Epoch 9 -- batch 0 / 514 loss: 0.08834689855575562
batch 500 / 514 loss: 0.09447869658470154
Epoch 10 -- batch 0 / 514 loss: 0.08943574875593185
batch 500 / 514 loss: 0.09380155056715012
Saving checkpoint at epoch 10; prefix = WN18RR-to-CoDExSmall-from-chkpt-ID_5360871134295512_e10-chkpt-ID_281109503281120
Done Training!

==================================
Testing (cite this): dataloader for dataset CoDExSmall
Testing (cite this): batch 0 / 29
batch 0 / 29 loss: 0.10321924090385437
Testing (cite this): batch 1 / 29
Testing (cite this): batch 2 / 29
Testing (cite this): batch 3 / 29
Testing (cite this): batch 4 / 29
Testing (cite this): batch 5 / 29
Testing (cite this): batch 6 / 29
Testing (cite this): batch 7 / 29
Testing (cite this): batch 8 / 29
Testing (cite this): batch 9 / 29
Testing (cite this): batch 10 / 29
Testing (cite this): batch 11 / 29
Testing (cite this): batch 12 / 29
Testing (cite this): batch 13 / 29
Testing (cite this): batch 14 / 29
Testing (cite this): batch 15 / 29
Testing (cite this): batch 16 / 29
Testing (cite this): batch 17 / 29
Testing (cite this): batch 18 / 29
Testing (cite this): batch 19 / 29
Testing (cite this): batch 20 / 29
Testing (cite this): batch 21 / 29
Testing (cite this): batch 22 / 29
Testing (cite this): batch 23 / 29
Testing (cite this): batch 24 / 29
Testing (cite this): batch 25 / 29
Testing (cite this): batch 26 / 29
Testing (cite this): batch 27 / 29
Testing (cite this): batch 28 / 29
total number of ranks, torch.Size([3656])
====== Ranks ======
ranks size: torch.Size([3656])
test_loss: 3.1478317007422447
mr: 1592.947021484375
mrr: 0.022396571934223175
h1: 0.02106126956641674
h3: 0.021608315408229828
h5: 0.021881837397813797
h10: 0.022975929081439972
==================================

Done Testing!
done with training and eval
loading model settings from cache: checkpoints/WN18RR-pretrain-chkpt-ID_5360871134295512.pkl
Running job! The arguemnts recieved are:
	 dataset_names: DBpedia50
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
	 optimizer_args: {'lr': 0.005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 10, 'npp': 30, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: WN18RR-to-DBpedia50-from-chkpt-ID_5360871134295512_e10

loading dataset
DBpedia50
X_p: torch.Size([32203, 22])
n_local: 21
X_p: torch.Size([2095, 22])
n_local: 21
X_p: torch.Size([123, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 252 loss: 0.06348348408937454
Epoch 2 -- batch 0 / 252 loss: 0.06372316181659698
Epoch 3 -- batch 0 / 252 loss: 0.06382164359092712
Epoch 4 -- batch 0 / 252 loss: 0.06483707576990128
Epoch 5 -- batch 0 / 252 loss: 0.06376615166664124
Saving checkpoint at epoch 5; prefix = WN18RR-to-DBpedia50-from-chkpt-ID_5360871134295512_e10-chkpt-ID_3158735827154503
Epoch 6 -- batch 0 / 252 loss: 0.0641174465417862
Epoch 7 -- batch 0 / 252 loss: 0.06392116844654083
Epoch 8 -- batch 0 / 252 loss: 0.06308556348085403
Epoch 9 -- batch 0 / 252 loss: 0.06314871460199356
Epoch 10 -- batch 0 / 252 loss: 0.06511920690536499
Saving checkpoint at epoch 10; prefix = WN18RR-to-DBpedia50-from-chkpt-ID_5360871134295512_e10-chkpt-ID_3158735827154503
Done Training!

==================================
Testing (cite this): dataloader for dataset DBpedia50
Testing (cite this): batch 0 / 33
batch 0 / 33 loss: 0.069083571434021
Testing (cite this): batch 1 / 33
Testing (cite this): batch 2 / 33
Testing (cite this): batch 3 / 33
Testing (cite this): batch 4 / 33
Testing (cite this): batch 5 / 33
Testing (cite this): batch 6 / 33
Testing (cite this): batch 7 / 33
Testing (cite this): batch 8 / 33
Testing (cite this): batch 9 / 33
Testing (cite this): batch 10 / 33
Testing (cite this): batch 11 / 33
Testing (cite this): batch 12 / 33
Testing (cite this): batch 13 / 33
Testing (cite this): batch 14 / 33
Testing (cite this): batch 15 / 33
Testing (cite this): batch 16 / 33
Testing (cite this): batch 17 / 33
Testing (cite this): batch 18 / 33
Testing (cite this): batch 19 / 33
Testing (cite this): batch 20 / 33
Testing (cite this): batch 21 / 33
Testing (cite this): batch 22 / 33
Testing (cite this): batch 23 / 33
Testing (cite this): batch 24 / 33
Testing (cite this): batch 25 / 33
Testing (cite this): batch 26 / 33
Testing (cite this): batch 27 / 33
Testing (cite this): batch 28 / 33
Testing (cite this): batch 29 / 33
Testing (cite this): batch 30 / 33
Testing (cite this): batch 31 / 33
Testing (cite this): batch 32 / 33
total number of ranks, torch.Size([4190])
====== Ranks ======
ranks size: torch.Size([4190])
test_loss: 2.677550122141838
mr: 7421.46923828125
mrr: 0.23311907052993774
h1: 0.21551312506198883
h3: 0.23770883679389954
h5: 0.2491646707057953
h10: 0.26539379358291626
==================================

Done Testing!
done with training and eval
loading model settings from cache: checkpoints/WN18RR-pretrain-chkpt-ID_5360871134295512.pkl
Running job! The arguemnts recieved are:
	 dataset_names: FB15k237
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
	 optimizer_args: {'lr': 0.0005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 10, 'npp': 100, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: WN18RR-to-FB15k237-from-chkpt-ID_5360871134295512_e10

loading dataset
FB15k237
X_p: torch.Size([272115, 22])
n_local: 21
X_p: torch.Size([20438, 22])
n_local: 21
X_p: torch.Size([17526, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 2126 loss: 0.0937267392873764
batch 500 / 2126 loss: 0.07968701422214508
batch 1000 / 2126 loss: 0.10251561552286148
batch 1500 / 2126 loss: 0.06044255942106247
batch 2000 / 2126 loss: 0.06802839040756226
Epoch 2 -- batch 0 / 2126 loss: 0.09287290275096893
batch 500 / 2126 loss: 0.08024795353412628
batch 1000 / 2126 loss: 0.10043799877166748
batch 1500 / 2126 loss: 0.06154888868331909
batch 2000 / 2126 loss: 0.0695347934961319
Epoch 3 -- batch 0 / 2126 loss: 0.09172949194908142
batch 500 / 2126 loss: 0.07965300232172012
batch 1000 / 2126 loss: 0.10152186453342438
batch 1500 / 2126 loss: 0.06146315112709999
batch 2000 / 2126 loss: 0.0677630826830864
Epoch 4 -- batch 0 / 2126 loss: 0.09205325692892075
batch 500 / 2126 loss: 0.07986856997013092
batch 1000 / 2126 loss: 0.10074581205844879
batch 1500 / 2126 loss: 0.06028708443045616
batch 2000 / 2126 loss: 0.07002375274896622
Epoch 5 -- batch 0 / 2126 loss: 0.09232507646083832
batch 500 / 2126 loss: 0.08064889907836914
batch 1000 / 2126 loss: 0.10109059512615204
batch 1500 / 2126 loss: 0.06093408167362213
batch 2000 / 2126 loss: 0.0689597800374031
Saving checkpoint at epoch 5; prefix = WN18RR-to-FB15k237-from-chkpt-ID_5360871134295512_e10-chkpt-ID_876532717782158
Epoch 6 -- batch 0 / 2126 loss: 0.09109899401664734
batch 500 / 2126 loss: 0.07927091419696808
batch 1000 / 2126 loss: 0.10084299743175507
batch 1500 / 2126 loss: 0.06061911582946777
batch 2000 / 2126 loss: 0.06788735836744308
Epoch 7 -- batch 0 / 2126 loss: 0.09053529798984528
batch 500 / 2126 loss: 0.0798790454864502
batch 1000 / 2126 loss: 0.10098274052143097
batch 1500 / 2126 loss: 0.06009862571954727
batch 2000 / 2126 loss: 0.06710737943649292
Epoch 8 -- batch 0 / 2126 loss: 0.09274296462535858
batch 500 / 2126 loss: 0.0796254426240921
batch 1000 / 2126 loss: 0.10021428763866425
batch 1500 / 2126 loss: 0.058404531329870224
batch 2000 / 2126 loss: 0.06830980628728867
Epoch 9 -- batch 0 / 2126 loss: 0.0928809642791748
batch 500 / 2126 loss: 0.07989418506622314
batch 1000 / 2126 loss: 0.10031338781118393
batch 1500 / 2126 loss: 0.06074255704879761
batch 2000 / 2126 loss: 0.06936778128147125
Epoch 10 -- batch 0 / 2126 loss: 0.09372211247682571
batch 500 / 2126 loss: 0.07972012460231781
batch 1000 / 2126 loss: 0.10138291865587234
batch 1500 / 2126 loss: 0.05999960005283356
batch 2000 / 2126 loss: 0.06794494390487671
Saving checkpoint at epoch 10; prefix = WN18RR-to-FB15k237-from-chkpt-ID_5360871134295512_e10-chkpt-ID_876532717782158
Done Training!

==================================
Testing (cite this): dataloader for dataset FB15k237
Testing (cite this): batch 0 / 320
batch 0 / 320 loss: 0.10533475875854492
Testing (cite this): batch 1 / 320
Testing (cite this): batch 2 / 320
Testing (cite this): batch 3 / 320
Testing (cite this): batch 4 / 320
Testing (cite this): batch 5 / 320
Testing (cite this): batch 6 / 320
Testing (cite this): batch 7 / 320
Testing (cite this): batch 8 / 320
Testing (cite this): batch 9 / 320
Testing (cite this): batch 10 / 320
Testing (cite this): batch 11 / 320
Testing (cite this): batch 12 / 320
Testing (cite this): batch 13 / 320
Testing (cite this): batch 14 / 320
Testing (cite this): batch 15 / 320
Testing (cite this): batch 16 / 320
Testing (cite this): batch 17 / 320
Testing (cite this): batch 18 / 320
Testing (cite this): batch 19 / 320
Testing (cite this): batch 20 / 320
Testing (cite this): batch 21 / 320
Testing (cite this): batch 22 / 320
Testing (cite this): batch 23 / 320
Testing (cite this): batch 24 / 320
Testing (cite this): batch 25 / 320
Testing (cite this): batch 26 / 320
Testing (cite this): batch 27 / 320
Testing (cite this): batch 28 / 320
Testing (cite this): batch 29 / 320
Testing (cite this): batch 30 / 320
Testing (cite this): batch 31 / 320
Testing (cite this): batch 32 / 320
Testing (cite this): batch 33 / 320
Testing (cite this): batch 34 / 320
Testing (cite this): batch 35 / 320
Testing (cite this): batch 36 / 320
Testing (cite this): batch 37 / 320
Testing (cite this): batch 38 / 320
Testing (cite this): batch 39 / 320
Testing (cite this): batch 40 / 320
Testing (cite this): batch 41 / 320
Testing (cite this): batch 42 / 320
Testing (cite this): batch 43 / 320
Testing (cite this): batch 44 / 320
Testing (cite this): batch 45 / 320
Testing (cite this): batch 46 / 320
Testing (cite this): batch 47 / 320
Testing (cite this): batch 48 / 320
Testing (cite this): batch 49 / 320
Testing (cite this): batch 50 / 320
Testing (cite this): batch 51 / 320
Testing (cite this): batch 52 / 320
Testing (cite this): batch 53 / 320
Testing (cite this): batch 54 / 320
Testing (cite this): batch 55 / 320
Testing (cite this): batch 56 / 320
Testing (cite this): batch 57 / 320
Testing (cite this): batch 58 / 320
Testing (cite this): batch 59 / 320
Testing (cite this): batch 60 / 320
Testing (cite this): batch 61 / 320
Testing (cite this): batch 62 / 320
Testing (cite this): batch 63 / 320
Testing (cite this): batch 64 / 320
Testing (cite this): batch 65 / 320
Testing (cite this): batch 66 / 320
Testing (cite this): batch 67 / 320
Testing (cite this): batch 68 / 320
Testing (cite this): batch 69 / 320
Testing (cite this): batch 70 / 320
Testing (cite this): batch 71 / 320
Testing (cite this): batch 72 / 320
Testing (cite this): batch 73 / 320
Testing (cite this): batch 74 / 320
Testing (cite this): batch 75 / 320
Testing (cite this): batch 76 / 320
Testing (cite this): batch 77 / 320
Testing (cite this): batch 78 / 320
Testing (cite this): batch 79 / 320
Testing (cite this): batch 80 / 320
Testing (cite this): batch 81 / 320
Testing (cite this): batch 82 / 320
Testing (cite this): batch 83 / 320
Testing (cite this): batch 84 / 320
Testing (cite this): batch 85 / 320
Testing (cite this): batch 86 / 320
Testing (cite this): batch 87 / 320
Testing (cite this): batch 88 / 320
Testing (cite this): batch 89 / 320
Testing (cite this): batch 90 / 320
Testing (cite this): batch 91 / 320
Testing (cite this): batch 92 / 320
Testing (cite this): batch 93 / 320
Testing (cite this): batch 94 / 320
Testing (cite this): batch 95 / 320
Testing (cite this): batch 96 / 320
Testing (cite this): batch 97 / 320
Testing (cite this): batch 98 / 320
Testing (cite this): batch 99 / 320
Testing (cite this): batch 100 / 320
Testing (cite this): batch 101 / 320
Testing (cite this): batch 102 / 320
Testing (cite this): batch 103 / 320
Testing (cite this): batch 104 / 320
Testing (cite this): batch 105 / 320
Testing (cite this): batch 106 / 320
Testing (cite this): batch 107 / 320
Testing (cite this): batch 108 / 320
Testing (cite this): batch 109 / 320
Testing (cite this): batch 110 / 320
Testing (cite this): batch 111 / 320
Testing (cite this): batch 112 / 320
Testing (cite this): batch 113 / 320
Testing (cite this): batch 114 / 320
Testing (cite this): batch 115 / 320
Testing (cite this): batch 116 / 320
Testing (cite this): batch 117 / 320
Testing (cite this): batch 118 / 320
Testing (cite this): batch 119 / 320
Testing (cite this): batch 120 / 320
Testing (cite this): batch 121 / 320
Testing (cite this): batch 122 / 320
Testing (cite this): batch 123 / 320
Testing (cite this): batch 124 / 320
Testing (cite this): batch 125 / 320
Testing (cite this): batch 126 / 320
Testing (cite this): batch 127 / 320
Testing (cite this): batch 128 / 320
Testing (cite this): batch 129 / 320
Testing (cite this): batch 130 / 320
Testing (cite this): batch 131 / 320
Testing (cite this): batch 132 / 320
Testing (cite this): batch 133 / 320
Testing (cite this): batch 134 / 320
Testing (cite this): batch 135 / 320
Testing (cite this): batch 136 / 320
Testing (cite this): batch 137 / 320
Testing (cite this): batch 138 / 320
Testing (cite this): batch 139 / 320
Testing (cite this): batch 140 / 320
Testing (cite this): batch 141 / 320
Testing (cite this): batch 142 / 320
Testing (cite this): batch 143 / 320
Testing (cite this): batch 144 / 320
Testing (cite this): batch 145 / 320
Testing (cite this): batch 146 / 320
Testing (cite this): batch 147 / 320
Testing (cite this): batch 148 / 320
Testing (cite this): batch 149 / 320
Testing (cite this): batch 150 / 320
Testing (cite this): batch 151 / 320
Testing (cite this): batch 152 / 320
Testing (cite this): batch 153 / 320
Testing (cite this): batch 154 / 320
Testing (cite this): batch 155 / 320
Testing (cite this): batch 156 / 320
Testing (cite this): batch 157 / 320
Testing (cite this): batch 158 / 320
Testing (cite this): batch 159 / 320
Testing (cite this): batch 160 / 320
Testing (cite this): batch 161 / 320
Testing (cite this): batch 162 / 320
Testing (cite this): batch 163 / 320
Testing (cite this): batch 164 / 320
Testing (cite this): batch 165 / 320
Testing (cite this): batch 166 / 320
Testing (cite this): batch 167 / 320
Testing (cite this): batch 168 / 320
Testing (cite this): batch 169 / 320
Testing (cite this): batch 170 / 320
Testing (cite this): batch 171 / 320
Testing (cite this): batch 172 / 320
Testing (cite this): batch 173 / 320
Testing (cite this): batch 174 / 320
Testing (cite this): batch 175 / 320
Testing (cite this): batch 176 / 320
Testing (cite this): batch 177 / 320
Testing (cite this): batch 178 / 320
Testing (cite this): batch 179 / 320
Testing (cite this): batch 180 / 320
Testing (cite this): batch 181 / 320
Testing (cite this): batch 182 / 320
Testing (cite this): batch 183 / 320
Testing (cite this): batch 184 / 320
Testing (cite this): batch 185 / 320
Testing (cite this): batch 186 / 320
Testing (cite this): batch 187 / 320
Testing (cite this): batch 188 / 320
Testing (cite this): batch 189 / 320
Testing (cite this): batch 190 / 320
Testing (cite this): batch 191 / 320
Testing (cite this): batch 192 / 320
Testing (cite this): batch 193 / 320
Testing (cite this): batch 194 / 320
Testing (cite this): batch 195 / 320
Testing (cite this): batch 196 / 320
Testing (cite this): batch 197 / 320
Testing (cite this): batch 198 / 320
Testing (cite this): batch 199 / 320
Testing (cite this): batch 200 / 320
Testing (cite this): batch 201 / 320
Testing (cite this): batch 202 / 320
Testing (cite this): batch 203 / 320
Testing (cite this): batch 204 / 320
Testing (cite this): batch 205 / 320
Testing (cite this): batch 206 / 320
Testing (cite this): batch 207 / 320
Testing (cite this): batch 208 / 320
Testing (cite this): batch 209 / 320
Testing (cite this): batch 210 / 320
Testing (cite this): batch 211 / 320
Testing (cite this): batch 212 / 320
Testing (cite this): batch 213 / 320
Testing (cite this): batch 214 / 320
Testing (cite this): batch 215 / 320
Testing (cite this): batch 216 / 320
Testing (cite this): batch 217 / 320
Testing (cite this): batch 218 / 320
Testing (cite this): batch 219 / 320
Testing (cite this): batch 220 / 320
Testing (cite this): batch 221 / 320
Testing (cite this): batch 222 / 320
Testing (cite this): batch 223 / 320
Testing (cite this): batch 224 / 320
Testing (cite this): batch 225 / 320
Testing (cite this): batch 226 / 320
Testing (cite this): batch 227 / 320
Testing (cite this): batch 228 / 320
Testing (cite this): batch 229 / 320
Testing (cite this): batch 230 / 320
Testing (cite this): batch 231 / 320
Testing (cite this): batch 232 / 320
Testing (cite this): batch 233 / 320
Testing (cite this): batch 234 / 320
Testing (cite this): batch 235 / 320
Testing (cite this): batch 236 / 320
Testing (cite this): batch 237 / 320
Testing (cite this): batch 238 / 320
Testing (cite this): batch 239 / 320
Testing (cite this): batch 240 / 320
Testing (cite this): batch 241 / 320
Testing (cite this): batch 242 / 320
Testing (cite this): batch 243 / 320
Testing (cite this): batch 244 / 320
Testing (cite this): batch 245 / 320
Testing (cite this): batch 246 / 320
Testing (cite this): batch 247 / 320
Testing (cite this): batch 248 / 320
Testing (cite this): batch 249 / 320
Testing (cite this): batch 250 / 320
Testing (cite this): batch 251 / 320
Testing (cite this): batch 252 / 320
Testing (cite this): batch 253 / 320
Testing (cite this): batch 254 / 320
Testing (cite this): batch 255 / 320
Testing (cite this): batch 256 / 320
Testing (cite this): batch 257 / 320
Testing (cite this): batch 258 / 320
Testing (cite this): batch 259 / 320
Testing (cite this): batch 260 / 320
Testing (cite this): batch 261 / 320
Testing (cite this): batch 262 / 320
Testing (cite this): batch 263 / 320
Testing (cite this): batch 264 / 320
Testing (cite this): batch 265 / 320
Testing (cite this): batch 266 / 320
Testing (cite this): batch 267 / 320
Testing (cite this): batch 268 / 320
Testing (cite this): batch 269 / 320
Testing (cite this): batch 270 / 320
Testing (cite this): batch 271 / 320
Testing (cite this): batch 272 / 320
Testing (cite this): batch 273 / 320
Testing (cite this): batch 274 / 320
Testing (cite this): batch 275 / 320
Testing (cite this): batch 276 / 320
Testing (cite this): batch 277 / 320
Testing (cite this): batch 278 / 320
Testing (cite this): batch 279 / 320
Testing (cite this): batch 280 / 320
Testing (cite this): batch 281 / 320
Testing (cite this): batch 282 / 320
Testing (cite this): batch 283 / 320
Testing (cite this): batch 284 / 320
Testing (cite this): batch 285 / 320
Testing (cite this): batch 286 / 320
Testing (cite this): batch 287 / 320
Testing (cite this): batch 288 / 320
Testing (cite this): batch 289 / 320
Testing (cite this): batch 290 / 320
Testing (cite this): batch 291 / 320
Testing (cite this): batch 292 / 320
Testing (cite this): batch 293 / 320
Testing (cite this): batch 294 / 320
Testing (cite this): batch 295 / 320
Testing (cite this): batch 296 / 320
Testing (cite this): batch 297 / 320
Testing (cite this): batch 298 / 320
Testing (cite this): batch 299 / 320
Testing (cite this): batch 300 / 320
Testing (cite this): batch 301 / 320
Testing (cite this): batch 302 / 320
Testing (cite this): batch 303 / 320
Testing (cite this): batch 304 / 320
Testing (cite this): batch 305 / 320
Testing (cite this): batch 306 / 320
Testing (cite this): batch 307 / 320
Testing (cite this): batch 308 / 320
Testing (cite this): batch 309 / 320
Testing (cite this): batch 310 / 320
Testing (cite this): batch 311 / 320
Testing (cite this): batch 312 / 320
Testing (cite this): batch 313 / 320
Testing (cite this): batch 314 / 320
Testing (cite this): batch 315 / 320
Testing (cite this): batch 316 / 320
Testing (cite this): batch 317 / 320
Testing (cite this): batch 318 / 320
Testing (cite this): batch 319 / 320
total number of ranks, torch.Size([40876])
====== Ranks ======
ranks size: torch.Size([40876])
test_loss: 42.38684967905283
mr: 11772.2138671875
mrr: 0.04762266203761101
h1: 0.04562579467892647
h3: 0.047436147928237915
h5: 0.05029846355319023
h10: 0.05205988883972168
==================================

Done Testing!
done with training and eval
loading model settings from cache: checkpoints/CoDExSmall-pretrain-chkpt-ID_8182621431323017.pkl
Running job! The arguemnts recieved are:
	 dataset_names: DBpedia50
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
	 optimizer_args: {'lr': 0.005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 10, 'npp': 30, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: CoDExSmall-to-DBpedia50-from-chkpt-ID_8182621431323017_e10

loading dataset
DBpedia50
X_p: torch.Size([32203, 22])
n_local: 21
X_p: torch.Size([2095, 22])
n_local: 21
X_p: torch.Size([123, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 252 loss: 0.032619670033454895
Epoch 2 -- batch 0 / 252 loss: 0.029681431129574776
Epoch 3 -- batch 0 / 252 loss: 0.029119696468114853
Epoch 4 -- batch 0 / 252 loss: 0.033090028911828995
Epoch 5 -- batch 0 / 252 loss: 0.03231651708483696
Saving checkpoint at epoch 5; prefix = CoDExSmall-to-DBpedia50-from-chkpt-ID_8182621431323017_e10-chkpt-ID_9291884739954238
Epoch 6 -- batch 0 / 252 loss: 0.030939685180783272
Epoch 7 -- batch 0 / 252 loss: 0.030960360541939735
Epoch 8 -- batch 0 / 252 loss: 0.03440919890999794
Epoch 9 -- batch 0 / 252 loss: 0.030871599912643433
Epoch 10 -- batch 0 / 252 loss: 0.030400360003113747
Saving checkpoint at epoch 10; prefix = CoDExSmall-to-DBpedia50-from-chkpt-ID_8182621431323017_e10-chkpt-ID_9291884739954238
Done Training!

==================================
Testing (cite this): dataloader for dataset DBpedia50
Testing (cite this): batch 0 / 33
batch 0 / 33 loss: 0.11515182256698608
Testing (cite this): batch 1 / 33
Testing (cite this): batch 2 / 33
Testing (cite this): batch 3 / 33
Testing (cite this): batch 4 / 33
Testing (cite this): batch 5 / 33
Testing (cite this): batch 6 / 33
Testing (cite this): batch 7 / 33
Testing (cite this): batch 8 / 33
Testing (cite this): batch 9 / 33
Testing (cite this): batch 10 / 33
Testing (cite this): batch 11 / 33
Testing (cite this): batch 12 / 33
Testing (cite this): batch 13 / 33
Testing (cite this): batch 14 / 33
Testing (cite this): batch 15 / 33
Testing (cite this): batch 16 / 33
Testing (cite this): batch 17 / 33
Testing (cite this): batch 18 / 33
Testing (cite this): batch 19 / 33
Testing (cite this): batch 20 / 33
Testing (cite this): batch 21 / 33
Testing (cite this): batch 22 / 33
Testing (cite this): batch 23 / 33
Testing (cite this): batch 24 / 33
Testing (cite this): batch 25 / 33
Testing (cite this): batch 26 / 33
Testing (cite this): batch 27 / 33
Testing (cite this): batch 28 / 33
Testing (cite this): batch 29 / 33
Testing (cite this): batch 30 / 33
Testing (cite this): batch 31 / 33
Testing (cite this): batch 32 / 33
total number of ranks, torch.Size([4190])
====== Ranks ======
ranks size: torch.Size([4190])
test_loss: 4.085686728358269
mr: 6457.16748046875
mrr: 0.3747563362121582
h1: 0.32601431012153625
h3: 0.41431981325149536
h5: 0.42482098937034607
h10: 0.44105011224746704
==================================

Done Testing!
done with training and eval
loading model settings from cache: checkpoints/CoDExSmall-pretrain-chkpt-ID_8182621431323017.pkl
Running job! The arguemnts recieved are:
	 dataset_names: FB15k237
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
	 optimizer_args: {'lr': 0.0005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 10, 'npp': 100, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: CoDExSmall-to-FB15k237-from-chkpt-ID_8182621431323017_e10

loading dataset
FB15k237
X_p: torch.Size([272115, 22])
n_local: 21
X_p: torch.Size([20438, 22])
n_local: 21
X_p: torch.Size([17526, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 2126 loss: 0.05490240454673767
batch 500 / 2126 loss: 0.036711763590574265
batch 1000 / 2126 loss: 0.048133157193660736
batch 1500 / 2126 loss: 0.06026884540915489
batch 2000 / 2126 loss: 0.07084926962852478
Epoch 2 -- batch 0 / 2126 loss: 0.05145041644573212
batch 500 / 2126 loss: 0.03625156730413437
batch 1000 / 2126 loss: 0.047827061265707016
batch 1500 / 2126 loss: 0.059713371098041534
batch 2000 / 2126 loss: 0.069118931889534
Epoch 3 -- batch 0 / 2126 loss: 0.05274549126625061
batch 500 / 2126 loss: 0.03858092427253723
batch 1000 / 2126 loss: 0.048410139977931976
batch 1500 / 2126 loss: 0.05783257633447647
batch 2000 / 2126 loss: 0.06944870948791504
Epoch 4 -- batch 0 / 2126 loss: 0.05275646597146988
batch 500 / 2126 loss: 0.03459055349230766
batch 1000 / 2126 loss: 0.047769248485565186
batch 1500 / 2126 loss: 0.057831116020679474
batch 2000 / 2126 loss: 0.0740252435207367
Epoch 5 -- batch 0 / 2126 loss: 0.05192415043711662
batch 500 / 2126 loss: 0.03508026897907257
batch 1000 / 2126 loss: 0.05014719069004059
batch 1500 / 2126 loss: 0.06126809120178223
batch 2000 / 2126 loss: 0.06986808776855469
Saving checkpoint at epoch 5; prefix = CoDExSmall-to-FB15k237-from-chkpt-ID_8182621431323017_e10-chkpt-ID_5847370656243976
Epoch 6 -- batch 0 / 2126 loss: 0.052115537226200104
batch 500 / 2126 loss: 0.03582054376602173
batch 1000 / 2126 loss: 0.04907287284731865
batch 1500 / 2126 loss: 0.05880074203014374
batch 2000 / 2126 loss: 0.06997712701559067
Epoch 7 -- batch 0 / 2126 loss: 0.054677996784448624
batch 500 / 2126 loss: 0.033212993294000626
batch 1000 / 2126 loss: 0.04877766966819763
batch 1500 / 2126 loss: 0.05948266386985779
batch 2000 / 2126 loss: 0.07466159760951996
Epoch 8 -- batch 0 / 2126 loss: 0.05157589912414551
batch 500 / 2126 loss: 0.03723396733403206
batch 1000 / 2126 loss: 0.048357367515563965
batch 1500 / 2126 loss: 0.05957303196191788
batch 2000 / 2126 loss: 0.06965053081512451
Epoch 9 -- batch 0 / 2126 loss: 0.05215019732713699
batch 500 / 2126 loss: 0.035777196288108826
batch 1000 / 2126 loss: 0.04926666244864464
batch 1500 / 2126 loss: 0.057704463601112366
batch 2000 / 2126 loss: 0.07083652913570404
Epoch 10 -- batch 0 / 2126 loss: 0.05220071226358414
batch 500 / 2126 loss: 0.03417055308818817
batch 1000 / 2126 loss: 0.048869892954826355
batch 1500 / 2126 loss: 0.06060996279120445
batch 2000 / 2126 loss: 0.07365044206380844
Saving checkpoint at epoch 10; prefix = CoDExSmall-to-FB15k237-from-chkpt-ID_8182621431323017_e10-chkpt-ID_5847370656243976
Done Training!

==================================
Testing (cite this): dataloader for dataset FB15k237
Testing (cite this): batch 0 / 320
batch 0 / 320 loss: 0.02535099908709526
Testing (cite this): batch 1 / 320
Testing (cite this): batch 2 / 320
Testing (cite this): batch 3 / 320
Testing (cite this): batch 4 / 320
Testing (cite this): batch 5 / 320
Testing (cite this): batch 6 / 320
Testing (cite this): batch 7 / 320
Testing (cite this): batch 8 / 320
Testing (cite this): batch 9 / 320
Testing (cite this): batch 10 / 320
Testing (cite this): batch 11 / 320
Testing (cite this): batch 12 / 320
Testing (cite this): batch 13 / 320
Testing (cite this): batch 14 / 320
Testing (cite this): batch 15 / 320
Testing (cite this): batch 16 / 320
Testing (cite this): batch 17 / 320
Testing (cite this): batch 18 / 320
Testing (cite this): batch 19 / 320
Testing (cite this): batch 20 / 320
Testing (cite this): batch 21 / 320
Testing (cite this): batch 22 / 320
Testing (cite this): batch 23 / 320
Testing (cite this): batch 24 / 320
Testing (cite this): batch 25 / 320
Testing (cite this): batch 26 / 320
Testing (cite this): batch 27 / 320
Testing (cite this): batch 28 / 320
Testing (cite this): batch 29 / 320
Testing (cite this): batch 30 / 320
Testing (cite this): batch 31 / 320
Testing (cite this): batch 32 / 320
Testing (cite this): batch 33 / 320
Testing (cite this): batch 34 / 320
Testing (cite this): batch 35 / 320
Testing (cite this): batch 36 / 320
Testing (cite this): batch 37 / 320
Testing (cite this): batch 38 / 320
Testing (cite this): batch 39 / 320
Testing (cite this): batch 40 / 320
Testing (cite this): batch 41 / 320
Testing (cite this): batch 42 / 320
Testing (cite this): batch 43 / 320
Testing (cite this): batch 44 / 320
Testing (cite this): batch 45 / 320
Testing (cite this): batch 46 / 320
Testing (cite this): batch 47 / 320
Testing (cite this): batch 48 / 320
Testing (cite this): batch 49 / 320
Testing (cite this): batch 50 / 320
Testing (cite this): batch 51 / 320
Testing (cite this): batch 52 / 320
Testing (cite this): batch 53 / 320
Testing (cite this): batch 54 / 320
Testing (cite this): batch 55 / 320
Testing (cite this): batch 56 / 320
Testing (cite this): batch 57 / 320
Testing (cite this): batch 58 / 320
Testing (cite this): batch 59 / 320
Testing (cite this): batch 60 / 320
Testing (cite this): batch 61 / 320
Testing (cite this): batch 62 / 320
Testing (cite this): batch 63 / 320
Testing (cite this): batch 64 / 320
Testing (cite this): batch 65 / 320
Testing (cite this): batch 66 / 320
Testing (cite this): batch 67 / 320
Testing (cite this): batch 68 / 320
Testing (cite this): batch 69 / 320
Testing (cite this): batch 70 / 320
Testing (cite this): batch 71 / 320
Testing (cite this): batch 72 / 320
Testing (cite this): batch 73 / 320
Testing (cite this): batch 74 / 320
Testing (cite this): batch 75 / 320
Testing (cite this): batch 76 / 320
Testing (cite this): batch 77 / 320
Testing (cite this): batch 78 / 320
Testing (cite this): batch 79 / 320
Testing (cite this): batch 80 / 320
Testing (cite this): batch 81 / 320
Testing (cite this): batch 82 / 320
Testing (cite this): batch 83 / 320
Testing (cite this): batch 84 / 320
Testing (cite this): batch 85 / 320
Testing (cite this): batch 86 / 320
Testing (cite this): batch 87 / 320
Testing (cite this): batch 88 / 320
Testing (cite this): batch 89 / 320
Testing (cite this): batch 90 / 320
Testing (cite this): batch 91 / 320
Testing (cite this): batch 92 / 320
Testing (cite this): batch 93 / 320
Testing (cite this): batch 94 / 320
Testing (cite this): batch 95 / 320
Testing (cite this): batch 96 / 320
Testing (cite this): batch 97 / 320
Testing (cite this): batch 98 / 320
Testing (cite this): batch 99 / 320
Testing (cite this): batch 100 / 320
Testing (cite this): batch 101 / 320
Testing (cite this): batch 102 / 320
Testing (cite this): batch 103 / 320
Testing (cite this): batch 104 / 320
Testing (cite this): batch 105 / 320
Testing (cite this): batch 106 / 320
Testing (cite this): batch 107 / 320
Testing (cite this): batch 108 / 320
Testing (cite this): batch 109 / 320
Testing (cite this): batch 110 / 320
Testing (cite this): batch 111 / 320
Testing (cite this): batch 112 / 320
Testing (cite this): batch 113 / 320
Testing (cite this): batch 114 / 320
Testing (cite this): batch 115 / 320
Testing (cite this): batch 116 / 320
Testing (cite this): batch 117 / 320
Testing (cite this): batch 118 / 320
Testing (cite this): batch 119 / 320
Testing (cite this): batch 120 / 320
Testing (cite this): batch 121 / 320
Testing (cite this): batch 122 / 320
Testing (cite this): batch 123 / 320
Testing (cite this): batch 124 / 320
Testing (cite this): batch 125 / 320
Testing (cite this): batch 126 / 320
Testing (cite this): batch 127 / 320
Testing (cite this): batch 128 / 320
Testing (cite this): batch 129 / 320
Testing (cite this): batch 130 / 320
Testing (cite this): batch 131 / 320
Testing (cite this): batch 132 / 320
Testing (cite this): batch 133 / 320
Testing (cite this): batch 134 / 320
Testing (cite this): batch 135 / 320
Testing (cite this): batch 136 / 320
Testing (cite this): batch 137 / 320
Testing (cite this): batch 138 / 320
Testing (cite this): batch 139 / 320
Testing (cite this): batch 140 / 320
Testing (cite this): batch 141 / 320
Testing (cite this): batch 142 / 320
Testing (cite this): batch 143 / 320
Testing (cite this): batch 144 / 320
Testing (cite this): batch 145 / 320
Testing (cite this): batch 146 / 320
Testing (cite this): batch 147 / 320
Testing (cite this): batch 148 / 320
Testing (cite this): batch 149 / 320
Testing (cite this): batch 150 / 320
Testing (cite this): batch 151 / 320
Testing (cite this): batch 152 / 320
Testing (cite this): batch 153 / 320
Testing (cite this): batch 154 / 320
Testing (cite this): batch 155 / 320
Testing (cite this): batch 156 / 320
Testing (cite this): batch 157 / 320
Testing (cite this): batch 158 / 320
Testing (cite this): batch 159 / 320
Testing (cite this): batch 160 / 320
Testing (cite this): batch 161 / 320
Testing (cite this): batch 162 / 320
Testing (cite this): batch 163 / 320
Testing (cite this): batch 164 / 320
Testing (cite this): batch 165 / 320
Testing (cite this): batch 166 / 320
Testing (cite this): batch 167 / 320
Testing (cite this): batch 168 / 320
Testing (cite this): batch 169 / 320
Testing (cite this): batch 170 / 320
Testing (cite this): batch 171 / 320
Testing (cite this): batch 172 / 320
Testing (cite this): batch 173 / 320
Testing (cite this): batch 174 / 320
Testing (cite this): batch 175 / 320
Testing (cite this): batch 176 / 320
Testing (cite this): batch 177 / 320
Testing (cite this): batch 178 / 320
Testing (cite this): batch 179 / 320
Testing (cite this): batch 180 / 320
Testing (cite this): batch 181 / 320
Testing (cite this): batch 182 / 320
Testing (cite this): batch 183 / 320
Testing (cite this): batch 184 / 320
Testing (cite this): batch 185 / 320
Testing (cite this): batch 186 / 320
Testing (cite this): batch 187 / 320
Testing (cite this): batch 188 / 320
Testing (cite this): batch 189 / 320
Testing (cite this): batch 190 / 320
Testing (cite this): batch 191 / 320
Testing (cite this): batch 192 / 320
Testing (cite this): batch 193 / 320
Testing (cite this): batch 194 / 320
Testing (cite this): batch 195 / 320
Testing (cite this): batch 196 / 320
Testing (cite this): batch 197 / 320
Testing (cite this): batch 198 / 320
Testing (cite this): batch 199 / 320
Testing (cite this): batch 200 / 320
Testing (cite this): batch 201 / 320
Testing (cite this): batch 202 / 320
Testing (cite this): batch 203 / 320
Testing (cite this): batch 204 / 320
Testing (cite this): batch 205 / 320
Testing (cite this): batch 206 / 320
Testing (cite this): batch 207 / 320
Testing (cite this): batch 208 / 320
Testing (cite this): batch 209 / 320
Testing (cite this): batch 210 / 320
Testing (cite this): batch 211 / 320
Testing (cite this): batch 212 / 320
Testing (cite this): batch 213 / 320
Testing (cite this): batch 214 / 320
Testing (cite this): batch 215 / 320
Testing (cite this): batch 216 / 320
Testing (cite this): batch 217 / 320
Testing (cite this): batch 218 / 320
Testing (cite this): batch 219 / 320
Testing (cite this): batch 220 / 320
Testing (cite this): batch 221 / 320
Testing (cite this): batch 222 / 320
Testing (cite this): batch 223 / 320
Testing (cite this): batch 224 / 320
Testing (cite this): batch 225 / 320
Testing (cite this): batch 226 / 320
Testing (cite this): batch 227 / 320
Testing (cite this): batch 228 / 320
Testing (cite this): batch 229 / 320
Testing (cite this): batch 230 / 320
Testing (cite this): batch 231 / 320
Testing (cite this): batch 232 / 320
Testing (cite this): batch 233 / 320
Testing (cite this): batch 234 / 320
Testing (cite this): batch 235 / 320
Testing (cite this): batch 236 / 320
Testing (cite this): batch 237 / 320
Testing (cite this): batch 238 / 320
Testing (cite this): batch 239 / 320
Testing (cite this): batch 240 / 320
Testing (cite this): batch 241 / 320
Testing (cite this): batch 242 / 320
Testing (cite this): batch 243 / 320
Testing (cite this): batch 244 / 320
Testing (cite this): batch 245 / 320
Testing (cite this): batch 246 / 320
Testing (cite this): batch 247 / 320
Testing (cite this): batch 248 / 320
Testing (cite this): batch 249 / 320
Testing (cite this): batch 250 / 320
Testing (cite this): batch 251 / 320
Testing (cite this): batch 252 / 320
Testing (cite this): batch 253 / 320
Testing (cite this): batch 254 / 320
Testing (cite this): batch 255 / 320
Testing (cite this): batch 256 / 320
Testing (cite this): batch 257 / 320
Testing (cite this): batch 258 / 320
Testing (cite this): batch 259 / 320
Testing (cite this): batch 260 / 320
Testing (cite this): batch 261 / 320
Testing (cite this): batch 262 / 320
Testing (cite this): batch 263 / 320
Testing (cite this): batch 264 / 320
Testing (cite this): batch 265 / 320
Testing (cite this): batch 266 / 320
Testing (cite this): batch 267 / 320
Testing (cite this): batch 268 / 320
Testing (cite this): batch 269 / 320
Testing (cite this): batch 270 / 320
Testing (cite this): batch 271 / 320
Testing (cite this): batch 272 / 320
Testing (cite this): batch 273 / 320
Testing (cite this): batch 274 / 320
Testing (cite this): batch 275 / 320
Testing (cite this): batch 276 / 320
Testing (cite this): batch 277 / 320
Testing (cite this): batch 278 / 320
Testing (cite this): batch 279 / 320
Testing (cite this): batch 280 / 320
Testing (cite this): batch 281 / 320
Testing (cite this): batch 282 / 320
Testing (cite this): batch 283 / 320
Testing (cite this): batch 284 / 320
Testing (cite this): batch 285 / 320
Testing (cite this): batch 286 / 320
Testing (cite this): batch 287 / 320
Testing (cite this): batch 288 / 320
Testing (cite this): batch 289 / 320
Testing (cite this): batch 290 / 320
Testing (cite this): batch 291 / 320
Testing (cite this): batch 292 / 320
Testing (cite this): batch 293 / 320
Testing (cite this): batch 294 / 320
Testing (cite this): batch 295 / 320
Testing (cite this): batch 296 / 320
Testing (cite this): batch 297 / 320
Testing (cite this): batch 298 / 320
Testing (cite this): batch 299 / 320
Testing (cite this): batch 300 / 320
Testing (cite this): batch 301 / 320
Testing (cite this): batch 302 / 320
Testing (cite this): batch 303 / 320
Testing (cite this): batch 304 / 320
Testing (cite this): batch 305 / 320
Testing (cite this): batch 306 / 320
Testing (cite this): batch 307 / 320
Testing (cite this): batch 308 / 320
Testing (cite this): batch 309 / 320
Testing (cite this): batch 310 / 320
Testing (cite this): batch 311 / 320
Testing (cite this): batch 312 / 320
Testing (cite this): batch 313 / 320
Testing (cite this): batch 314 / 320
Testing (cite this): batch 315 / 320
Testing (cite this): batch 316 / 320
Testing (cite this): batch 317 / 320
Testing (cite this): batch 318 / 320
Testing (cite this): batch 319 / 320
total number of ranks, torch.Size([40876])
====== Ranks ======
ranks size: torch.Size([40876])
test_loss: 9.854573797812918
mr: 1303.01708984375
mrr: 0.41278472542762756
h1: 0.3595753014087677
h3: 0.4072071611881256
h5: 0.5059692859649658
h10: 0.5663470029830933
==================================

Done Testing!
done with training and eval
loading model settings from cache: checkpoints/CoDExSmall-pretrain-chkpt-ID_8182621431323017.pkl
Running job! The arguemnts recieved are:
	 dataset_names: WN18RR
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
	 optimizer_args: {'lr': 0.005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 10, 'npp': 500, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: CoDExSmall-to-WN18RR-from-chkpt-ID_8182621431323017_e10

loading dataset
WN18RR
X_p: torch.Size([86835, 22])
n_local: 21
X_p: torch.Size([2924, 22])
n_local: 21
X_p: torch.Size([2824, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 679 loss: 0.0733894556760788
batch 500 / 679 loss: 0.08562920987606049
Epoch 2 -- batch 0 / 679 loss: 0.07609029859304428
batch 500 / 679 loss: 0.08675751090049744
Epoch 3 -- batch 0 / 679 loss: 0.07440868765115738
batch 500 / 679 loss: 0.08697908371686935
Epoch 4 -- batch 0 / 679 loss: 0.07492554187774658
batch 500 / 679 loss: 0.08750580996274948
Epoch 5 -- batch 0 / 679 loss: 0.0768376886844635
batch 500 / 679 loss: 0.08968837559223175
Saving checkpoint at epoch 5; prefix = CoDExSmall-to-WN18RR-from-chkpt-ID_8182621431323017_e10-chkpt-ID_1655186629075139
Epoch 6 -- batch 0 / 679 loss: 0.07518500089645386
batch 500 / 679 loss: 0.08826291561126709
Epoch 7 -- batch 0 / 679 loss: 0.07416757941246033
batch 500 / 679 loss: 0.08710108697414398
Epoch 8 -- batch 0 / 679 loss: 0.07609109580516815
batch 500 / 679 loss: 0.08864821493625641
Epoch 9 -- batch 0 / 679 loss: 0.07485264539718628
batch 500 / 679 loss: 0.08854900300502777
Epoch 10 -- batch 0 / 679 loss: 0.0759202390909195
batch 500 / 679 loss: 0.08947475254535675
Saving checkpoint at epoch 10; prefix = CoDExSmall-to-WN18RR-from-chkpt-ID_8182621431323017_e10-chkpt-ID_1655186629075139
Done Training!

==================================
Testing (cite this): dataloader for dataset WN18RR
Testing (cite this): batch 0 / 46
batch 0 / 46 loss: 0.034920915961265564
Testing (cite this): batch 1 / 46
Testing (cite this): batch 2 / 46
Testing (cite this): batch 3 / 46
Testing (cite this): batch 4 / 46
Testing (cite this): batch 5 / 46
Testing (cite this): batch 6 / 46
Testing (cite this): batch 7 / 46
Testing (cite this): batch 8 / 46
Testing (cite this): batch 9 / 46
Testing (cite this): batch 10 / 46
Testing (cite this): batch 11 / 46
Testing (cite this): batch 12 / 46
Testing (cite this): batch 13 / 46
Testing (cite this): batch 14 / 46
Testing (cite this): batch 15 / 46
Testing (cite this): batch 16 / 46
Testing (cite this): batch 17 / 46
Testing (cite this): batch 18 / 46
Testing (cite this): batch 19 / 46
Testing (cite this): batch 20 / 46
Testing (cite this): batch 21 / 46
Testing (cite this): batch 22 / 46
Testing (cite this): batch 23 / 46
Testing (cite this): batch 24 / 46
Testing (cite this): batch 25 / 46
Testing (cite this): batch 26 / 46
Testing (cite this): batch 27 / 46
Testing (cite this): batch 28 / 46
Testing (cite this): batch 29 / 46
Testing (cite this): batch 30 / 46
Testing (cite this): batch 31 / 46
Testing (cite this): batch 32 / 46
Testing (cite this): batch 33 / 46
Testing (cite this): batch 34 / 46
Testing (cite this): batch 35 / 46
Testing (cite this): batch 36 / 46
Testing (cite this): batch 37 / 46
Testing (cite this): batch 38 / 46
Testing (cite this): batch 39 / 46
Testing (cite this): batch 40 / 46
Testing (cite this): batch 41 / 46
Testing (cite this): batch 42 / 46
Testing (cite this): batch 43 / 46
Testing (cite this): batch 44 / 46
Testing (cite this): batch 45 / 46
total number of ranks, torch.Size([5848])
====== Ranks ======
ranks size: torch.Size([5848])
test_loss: 2.3232825361192226
mr: 8076.52685546875
mrr: 0.34304776787757874
h1: 0.3122434914112091
h3: 0.37004104256629944
h5: 0.38337892293930054
h10: 0.39466485381126404
==================================

Done Testing!
done with training and eval
loading model settings from cache: checkpoints/DBpedia50-pretrain-chkpt-ID_2429873403687784.pkl
Running job! The arguemnts recieved are:
	 dataset_names: CoDExSmall
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
	 optimizer_args: {'lr': 0.005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 64, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 10, 'npp': 100, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: DBpedia50-to-CoDExSmall-from-chkpt-ID_2429873403687784_e10

loading dataset
CoDExSmall
X_p: torch.Size([32888, 22])
n_local: 21
X_p: torch.Size([1828, 22])
n_local: 21
X_p: torch.Size([1827, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 514 loss: 0.05475109815597534
batch 500 / 514 loss: 0.11108183860778809
Epoch 2 -- batch 0 / 514 loss: 0.056544333696365356
batch 500 / 514 loss: 0.11139829456806183
Epoch 3 -- batch 0 / 514 loss: 0.05499563366174698
batch 500 / 514 loss: 0.11192958801984787
Epoch 4 -- batch 0 / 514 loss: 0.05501776561141014
batch 500 / 514 loss: 0.10748893022537231
Epoch 5 -- batch 0 / 514 loss: 0.056332044303417206
batch 500 / 514 loss: 0.11020446568727493
Saving checkpoint at epoch 5; prefix = DBpedia50-to-CoDExSmall-from-chkpt-ID_2429873403687784_e10-chkpt-ID_9800075536900798
Epoch 6 -- batch 0 / 514 loss: 0.05669732391834259
batch 500 / 514 loss: 0.10893255472183228
Epoch 7 -- batch 0 / 514 loss: 0.05594304949045181
batch 500 / 514 loss: 0.11070926487445831
Epoch 8 -- batch 0 / 514 loss: 0.05536631494760513
batch 500 / 514 loss: 0.10891619324684143
Epoch 9 -- batch 0 / 514 loss: 0.05446651577949524
batch 500 / 514 loss: 0.10643652081489563
Epoch 10 -- batch 0 / 514 loss: 0.056462354958057404
batch 500 / 514 loss: 0.10995037108659744
Saving checkpoint at epoch 10; prefix = DBpedia50-to-CoDExSmall-from-chkpt-ID_2429873403687784_e10-chkpt-ID_9800075536900798
Done Training!

==================================
Testing (cite this): dataloader for dataset CoDExSmall
Testing (cite this): batch 0 / 29
batch 0 / 29 loss: 0.14301282167434692
Testing (cite this): batch 1 / 29
Testing (cite this): batch 2 / 29
Testing (cite this): batch 3 / 29
Testing (cite this): batch 4 / 29
Testing (cite this): batch 5 / 29
Testing (cite this): batch 6 / 29
Testing (cite this): batch 7 / 29
Testing (cite this): batch 8 / 29
Testing (cite this): batch 9 / 29
Testing (cite this): batch 10 / 29
Testing (cite this): batch 11 / 29
Testing (cite this): batch 12 / 29
Testing (cite this): batch 13 / 29
Testing (cite this): batch 14 / 29
Testing (cite this): batch 15 / 29
Testing (cite this): batch 16 / 29
Testing (cite this): batch 17 / 29
Testing (cite this): batch 18 / 29
Testing (cite this): batch 19 / 29
Testing (cite this): batch 20 / 29
Testing (cite this): batch 21 / 29
Testing (cite this): batch 22 / 29
Testing (cite this): batch 23 / 29
Testing (cite this): batch 24 / 29
Testing (cite this): batch 25 / 29
Testing (cite this): batch 26 / 29
Testing (cite this): batch 27 / 29
Testing (cite this): batch 28 / 29
total number of ranks, torch.Size([3656])
====== Ranks ======
ranks size: torch.Size([3656])
test_loss: 3.980195052921772
mr: 1619.189453125
mrr: 0.071904256939888
h1: 0.06783369928598404
h3: 0.07193654030561447
h5: 0.07439824938774109
h10: 0.07822757214307785
==================================

Done Testing!
done with training and eval
loading model settings from cache: checkpoints/DBpedia50-pretrain-chkpt-ID_2429873403687784.pkl
Running job! The arguemnts recieved are:
	 dataset_names: FB15k237
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
	 optimizer_args: {'lr': 0.0005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 10, 'npp': 100, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: DBpedia50-to-FB15k237-from-chkpt-ID_2429873403687784_e10

loading dataset
FB15k237
X_p: torch.Size([272115, 22])
n_local: 21
X_p: torch.Size([20438, 22])
n_local: 21
X_p: torch.Size([17526, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 2126 loss: 0.12224560230970383
batch 500 / 2126 loss: 0.1139313355088234
batch 1000 / 2126 loss: 0.05308902636170387
batch 1500 / 2126 loss: 0.08258020877838135
batch 2000 / 2126 loss: 0.07897277921438217
Epoch 2 -- batch 0 / 2126 loss: 0.12100714445114136
batch 500 / 2126 loss: 0.11308901011943817
batch 1000 / 2126 loss: 0.053952060639858246
batch 1500 / 2126 loss: 0.08503483980894089
batch 2000 / 2126 loss: 0.077318474650383
Epoch 3 -- batch 0 / 2126 loss: 0.12326296418905258
batch 500 / 2126 loss: 0.11278635263442993
batch 1000 / 2126 loss: 0.053087640553712845
batch 1500 / 2126 loss: 0.08412250131368637
batch 2000 / 2126 loss: 0.07986532151699066
Epoch 4 -- batch 0 / 2126 loss: 0.1223297119140625
batch 500 / 2126 loss: 0.11502227187156677
batch 1000 / 2126 loss: 0.05282532423734665
batch 1500 / 2126 loss: 0.08382979035377502
batch 2000 / 2126 loss: 0.08079089224338531
Epoch 5 -- batch 0 / 2126 loss: 0.12279793620109558
batch 500 / 2126 loss: 0.11240295320749283
batch 1000 / 2126 loss: 0.05298439413309097
batch 1500 / 2126 loss: 0.08355630189180374
batch 2000 / 2126 loss: 0.08017303049564362
Saving checkpoint at epoch 5; prefix = DBpedia50-to-FB15k237-from-chkpt-ID_2429873403687784_e10-chkpt-ID_8667710785001854
Epoch 6 -- batch 0 / 2126 loss: 0.12198197841644287
batch 500 / 2126 loss: 0.11468096077442169
batch 1000 / 2126 loss: 0.05364195257425308
batch 1500 / 2126 loss: 0.08497413247823715
batch 2000 / 2126 loss: 0.08010099083185196
Epoch 7 -- batch 0 / 2126 loss: 0.12110745906829834
batch 500 / 2126 loss: 0.11587733030319214
batch 1000 / 2126 loss: 0.05411839112639427
batch 1500 / 2126 loss: 0.08159414678812027
batch 2000 / 2126 loss: 0.0790657177567482
Epoch 8 -- batch 0 / 2126 loss: 0.12154650688171387
batch 500 / 2126 loss: 0.11425383388996124
batch 1000 / 2126 loss: 0.05350466072559357
batch 1500 / 2126 loss: 0.08552947640419006
batch 2000 / 2126 loss: 0.07993199676275253
Epoch 9 -- batch 0 / 2126 loss: 0.11941425502300262
batch 500 / 2126 loss: 0.11278541386127472
batch 1000 / 2126 loss: 0.053030483424663544
batch 1500 / 2126 loss: 0.08367049694061279
batch 2000 / 2126 loss: 0.08004981279373169
Epoch 10 -- batch 0 / 2126 loss: 0.12072285264730453
batch 500 / 2126 loss: 0.11472951620817184
batch 1000 / 2126 loss: 0.053322598338127136
batch 1500 / 2126 loss: 0.08602045476436615
batch 2000 / 2126 loss: 0.08119405061006546
Saving checkpoint at epoch 10; prefix = DBpedia50-to-FB15k237-from-chkpt-ID_2429873403687784_e10-chkpt-ID_8667710785001854
Done Training!

==================================
Testing (cite this): dataloader for dataset FB15k237
Testing (cite this): batch 0 / 320
batch 0 / 320 loss: 0.22907346487045288
Testing (cite this): batch 1 / 320
Testing (cite this): batch 2 / 320
Testing (cite this): batch 3 / 320
Testing (cite this): batch 4 / 320
Testing (cite this): batch 5 / 320
Testing (cite this): batch 6 / 320
Testing (cite this): batch 7 / 320
Testing (cite this): batch 8 / 320
Testing (cite this): batch 9 / 320
Testing (cite this): batch 10 / 320
Testing (cite this): batch 11 / 320
Testing (cite this): batch 12 / 320
Testing (cite this): batch 13 / 320
Testing (cite this): batch 14 / 320
Testing (cite this): batch 15 / 320
Testing (cite this): batch 16 / 320
Testing (cite this): batch 17 / 320
Testing (cite this): batch 18 / 320
Testing (cite this): batch 19 / 320
Testing (cite this): batch 20 / 320
Testing (cite this): batch 21 / 320
Testing (cite this): batch 22 / 320
Testing (cite this): batch 23 / 320
Testing (cite this): batch 24 / 320
Testing (cite this): batch 25 / 320
Testing (cite this): batch 26 / 320
Testing (cite this): batch 27 / 320
Testing (cite this): batch 28 / 320
Testing (cite this): batch 29 / 320
Testing (cite this): batch 30 / 320
Testing (cite this): batch 31 / 320
Testing (cite this): batch 32 / 320
Testing (cite this): batch 33 / 320
Testing (cite this): batch 34 / 320
Testing (cite this): batch 35 / 320
Testing (cite this): batch 36 / 320
Testing (cite this): batch 37 / 320
Testing (cite this): batch 38 / 320
Testing (cite this): batch 39 / 320
Testing (cite this): batch 40 / 320
Testing (cite this): batch 41 / 320
Testing (cite this): batch 42 / 320
Testing (cite this): batch 43 / 320
Testing (cite this): batch 44 / 320
Testing (cite this): batch 45 / 320
Testing (cite this): batch 46 / 320
Testing (cite this): batch 47 / 320
Testing (cite this): batch 48 / 320
Testing (cite this): batch 49 / 320
Testing (cite this): batch 50 / 320
Testing (cite this): batch 51 / 320
Testing (cite this): batch 52 / 320
Testing (cite this): batch 53 / 320
Testing (cite this): batch 54 / 320
Testing (cite this): batch 55 / 320
Testing (cite this): batch 56 / 320
Testing (cite this): batch 57 / 320
Testing (cite this): batch 58 / 320
Testing (cite this): batch 59 / 320
Testing (cite this): batch 60 / 320
Testing (cite this): batch 61 / 320
Testing (cite this): batch 62 / 320
Testing (cite this): batch 63 / 320
Testing (cite this): batch 64 / 320
Testing (cite this): batch 65 / 320
Testing (cite this): batch 66 / 320
Testing (cite this): batch 67 / 320
Testing (cite this): batch 68 / 320
Testing (cite this): batch 69 / 320
Testing (cite this): batch 70 / 320
Testing (cite this): batch 71 / 320
Testing (cite this): batch 72 / 320
Testing (cite this): batch 73 / 320
Testing (cite this): batch 74 / 320
Testing (cite this): batch 75 / 320
Testing (cite this): batch 76 / 320
Testing (cite this): batch 77 / 320
Testing (cite this): batch 78 / 320
Testing (cite this): batch 79 / 320
Testing (cite this): batch 80 / 320
Testing (cite this): batch 81 / 320
Testing (cite this): batch 82 / 320
Testing (cite this): batch 83 / 320
Testing (cite this): batch 84 / 320
Testing (cite this): batch 85 / 320
Testing (cite this): batch 86 / 320
Testing (cite this): batch 87 / 320
Testing (cite this): batch 88 / 320
Testing (cite this): batch 89 / 320
Testing (cite this): batch 90 / 320
Testing (cite this): batch 91 / 320
Testing (cite this): batch 92 / 320
Testing (cite this): batch 93 / 320
Testing (cite this): batch 94 / 320
Testing (cite this): batch 95 / 320
Testing (cite this): batch 96 / 320
Testing (cite this): batch 97 / 320
Testing (cite this): batch 98 / 320
Testing (cite this): batch 99 / 320
Testing (cite this): batch 100 / 320
Testing (cite this): batch 101 / 320
Testing (cite this): batch 102 / 320
Testing (cite this): batch 103 / 320
Testing (cite this): batch 104 / 320
Testing (cite this): batch 105 / 320
Testing (cite this): batch 106 / 320
Testing (cite this): batch 107 / 320
Testing (cite this): batch 108 / 320
Testing (cite this): batch 109 / 320
Testing (cite this): batch 110 / 320
Testing (cite this): batch 111 / 320
Testing (cite this): batch 112 / 320
Testing (cite this): batch 113 / 320
Testing (cite this): batch 114 / 320
Testing (cite this): batch 115 / 320
Testing (cite this): batch 116 / 320
Testing (cite this): batch 117 / 320
Testing (cite this): batch 118 / 320
Testing (cite this): batch 119 / 320
Testing (cite this): batch 120 / 320
Testing (cite this): batch 121 / 320
Testing (cite this): batch 122 / 320
Testing (cite this): batch 123 / 320
Testing (cite this): batch 124 / 320
Testing (cite this): batch 125 / 320
Testing (cite this): batch 126 / 320
Testing (cite this): batch 127 / 320
Testing (cite this): batch 128 / 320
Testing (cite this): batch 129 / 320
Testing (cite this): batch 130 / 320
Testing (cite this): batch 131 / 320
Testing (cite this): batch 132 / 320
Testing (cite this): batch 133 / 320
Testing (cite this): batch 134 / 320
Testing (cite this): batch 135 / 320
Testing (cite this): batch 136 / 320
Testing (cite this): batch 137 / 320
Testing (cite this): batch 138 / 320
Testing (cite this): batch 139 / 320
Testing (cite this): batch 140 / 320
Testing (cite this): batch 141 / 320
Testing (cite this): batch 142 / 320
Testing (cite this): batch 143 / 320
Testing (cite this): batch 144 / 320
Testing (cite this): batch 145 / 320
Testing (cite this): batch 146 / 320
Testing (cite this): batch 147 / 320
Testing (cite this): batch 148 / 320
Testing (cite this): batch 149 / 320
Testing (cite this): batch 150 / 320
Testing (cite this): batch 151 / 320
Testing (cite this): batch 152 / 320
Testing (cite this): batch 153 / 320
Testing (cite this): batch 154 / 320
Testing (cite this): batch 155 / 320
Testing (cite this): batch 156 / 320
Testing (cite this): batch 157 / 320
Testing (cite this): batch 158 / 320
Testing (cite this): batch 159 / 320
Testing (cite this): batch 160 / 320
Testing (cite this): batch 161 / 320
Testing (cite this): batch 162 / 320
Testing (cite this): batch 163 / 320
Testing (cite this): batch 164 / 320
Testing (cite this): batch 165 / 320
Testing (cite this): batch 166 / 320
Testing (cite this): batch 167 / 320
Testing (cite this): batch 168 / 320
Testing (cite this): batch 169 / 320
Testing (cite this): batch 170 / 320
Testing (cite this): batch 171 / 320
Testing (cite this): batch 172 / 320
Testing (cite this): batch 173 / 320
Testing (cite this): batch 174 / 320
Testing (cite this): batch 175 / 320
Testing (cite this): batch 176 / 320
Testing (cite this): batch 177 / 320
Testing (cite this): batch 178 / 320
Testing (cite this): batch 179 / 320
Testing (cite this): batch 180 / 320
Testing (cite this): batch 181 / 320
Testing (cite this): batch 182 / 320
Testing (cite this): batch 183 / 320
Testing (cite this): batch 184 / 320
Testing (cite this): batch 185 / 320
Testing (cite this): batch 186 / 320
Testing (cite this): batch 187 / 320
Testing (cite this): batch 188 / 320
Testing (cite this): batch 189 / 320
Testing (cite this): batch 190 / 320
Testing (cite this): batch 191 / 320
Testing (cite this): batch 192 / 320
Testing (cite this): batch 193 / 320
Testing (cite this): batch 194 / 320
Testing (cite this): batch 195 / 320
Testing (cite this): batch 196 / 320
Testing (cite this): batch 197 / 320
Testing (cite this): batch 198 / 320
Testing (cite this): batch 199 / 320
Testing (cite this): batch 200 / 320
Testing (cite this): batch 201 / 320
Testing (cite this): batch 202 / 320
Testing (cite this): batch 203 / 320
Testing (cite this): batch 204 / 320
Testing (cite this): batch 205 / 320
Testing (cite this): batch 206 / 320
Testing (cite this): batch 207 / 320
Testing (cite this): batch 208 / 320
Testing (cite this): batch 209 / 320
Testing (cite this): batch 210 / 320
Testing (cite this): batch 211 / 320
Testing (cite this): batch 212 / 320
Testing (cite this): batch 213 / 320
Testing (cite this): batch 214 / 320
Testing (cite this): batch 215 / 320
Testing (cite this): batch 216 / 320
Testing (cite this): batch 217 / 320
Testing (cite this): batch 218 / 320
Testing (cite this): batch 219 / 320
Testing (cite this): batch 220 / 320
Testing (cite this): batch 221 / 320
Testing (cite this): batch 222 / 320
Testing (cite this): batch 223 / 320
Testing (cite this): batch 224 / 320
Testing (cite this): batch 225 / 320
Testing (cite this): batch 226 / 320
Testing (cite this): batch 227 / 320
Testing (cite this): batch 228 / 320
Testing (cite this): batch 229 / 320
Testing (cite this): batch 230 / 320
Testing (cite this): batch 231 / 320
Testing (cite this): batch 232 / 320
Testing (cite this): batch 233 / 320
Testing (cite this): batch 234 / 320
Testing (cite this): batch 235 / 320
Testing (cite this): batch 236 / 320
Testing (cite this): batch 237 / 320
Testing (cite this): batch 238 / 320
Testing (cite this): batch 239 / 320
Testing (cite this): batch 240 / 320
Testing (cite this): batch 241 / 320
Testing (cite this): batch 242 / 320
Testing (cite this): batch 243 / 320
Testing (cite this): batch 244 / 320
Testing (cite this): batch 245 / 320
Testing (cite this): batch 246 / 320
Testing (cite this): batch 247 / 320
Testing (cite this): batch 248 / 320
Testing (cite this): batch 249 / 320
Testing (cite this): batch 250 / 320
Testing (cite this): batch 251 / 320
Testing (cite this): batch 252 / 320
Testing (cite this): batch 253 / 320
Testing (cite this): batch 254 / 320
Testing (cite this): batch 255 / 320
Testing (cite this): batch 256 / 320
Testing (cite this): batch 257 / 320
Testing (cite this): batch 258 / 320
Testing (cite this): batch 259 / 320
Testing (cite this): batch 260 / 320
Testing (cite this): batch 261 / 320
Testing (cite this): batch 262 / 320
Testing (cite this): batch 263 / 320
Testing (cite this): batch 264 / 320
Testing (cite this): batch 265 / 320
Testing (cite this): batch 266 / 320
Testing (cite this): batch 267 / 320
Testing (cite this): batch 268 / 320
Testing (cite this): batch 269 / 320
Testing (cite this): batch 270 / 320
Testing (cite this): batch 271 / 320
Testing (cite this): batch 272 / 320
Testing (cite this): batch 273 / 320
Testing (cite this): batch 274 / 320
Testing (cite this): batch 275 / 320
Testing (cite this): batch 276 / 320
Testing (cite this): batch 277 / 320
Testing (cite this): batch 278 / 320
Testing (cite this): batch 279 / 320
Testing (cite this): batch 280 / 320
Testing (cite this): batch 281 / 320
Testing (cite this): batch 282 / 320
Testing (cite this): batch 283 / 320
Testing (cite this): batch 284 / 320
Testing (cite this): batch 285 / 320
Testing (cite this): batch 286 / 320
Testing (cite this): batch 287 / 320
Testing (cite this): batch 288 / 320
Testing (cite this): batch 289 / 320
Testing (cite this): batch 290 / 320
Testing (cite this): batch 291 / 320
Testing (cite this): batch 292 / 320
Testing (cite this): batch 293 / 320
Testing (cite this): batch 294 / 320
Testing (cite this): batch 295 / 320
Testing (cite this): batch 296 / 320
Testing (cite this): batch 297 / 320
Testing (cite this): batch 298 / 320
Testing (cite this): batch 299 / 320
Testing (cite this): batch 300 / 320
Testing (cite this): batch 301 / 320
Testing (cite this): batch 302 / 320
Testing (cite this): batch 303 / 320
Testing (cite this): batch 304 / 320
Testing (cite this): batch 305 / 320
Testing (cite this): batch 306 / 320
Testing (cite this): batch 307 / 320
Testing (cite this): batch 308 / 320
Testing (cite this): batch 309 / 320
Testing (cite this): batch 310 / 320
Testing (cite this): batch 311 / 320
Testing (cite this): batch 312 / 320
Testing (cite this): batch 313 / 320
Testing (cite this): batch 314 / 320
Testing (cite this): batch 315 / 320
Testing (cite this): batch 316 / 320
Testing (cite this): batch 317 / 320
Testing (cite this): batch 318 / 320
Testing (cite this): batch 319 / 320
total number of ranks, torch.Size([40876])
====== Ranks ======
ranks size: torch.Size([40876])
test_loss: 103.50242181494832
mr: 10230.369140625
mrr: 0.14614586532115936
h1: 0.14233291149139404
h3: 0.1472257524728775
h5: 0.15018592774868011
h10: 0.1539289504289627
==================================

Done Testing!
done with training and eval
loading model settings from cache: checkpoints/DBpedia50-pretrain-chkpt-ID_2429873403687784.pkl
Running job! The arguemnts recieved are:
	 dataset_names: WN18RR
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.005
    maximize: False
    weight_decay: 0
)
	 optimizer_args: {'lr': 0.005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 10, 'npp': 500, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: DBpedia50-to-WN18RR-from-chkpt-ID_2429873403687784_e10

loading dataset
WN18RR
X_p: torch.Size([86835, 22])
n_local: 21
X_p: torch.Size([2924, 22])
n_local: 21
X_p: torch.Size([2824, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 679 loss: 0.06583236157894135
batch 500 / 679 loss: 0.02640598639845848
Epoch 2 -- batch 0 / 679 loss: 0.06940236687660217
batch 500 / 679 loss: 0.025401107966899872
Epoch 3 -- batch 0 / 679 loss: 0.06643933802843094
batch 500 / 679 loss: 0.02661195397377014
Epoch 4 -- batch 0 / 679 loss: 0.06661999225616455
batch 500 / 679 loss: 0.02732500247657299
Epoch 5 -- batch 0 / 679 loss: 0.06522974371910095
batch 500 / 679 loss: 0.028074324131011963
Saving checkpoint at epoch 5; prefix = DBpedia50-to-WN18RR-from-chkpt-ID_2429873403687784_e10-chkpt-ID_8053998299591917
Epoch 6 -- batch 0 / 679 loss: 0.0674312487244606
batch 500 / 679 loss: 0.026567505672574043
Epoch 7 -- batch 0 / 679 loss: 0.0666443258523941
batch 500 / 679 loss: 0.02637852169573307
Epoch 8 -- batch 0 / 679 loss: 0.0652446299791336
batch 500 / 679 loss: 0.027435436844825745
Epoch 9 -- batch 0 / 679 loss: 0.06676878780126572
batch 500 / 679 loss: 0.025615312159061432
Epoch 10 -- batch 0 / 679 loss: 0.06687511503696442
batch 500 / 679 loss: 0.028193451464176178
Saving checkpoint at epoch 10; prefix = DBpedia50-to-WN18RR-from-chkpt-ID_2429873403687784_e10-chkpt-ID_8053998299591917
Done Training!

==================================
Testing (cite this): dataloader for dataset WN18RR
Testing (cite this): batch 0 / 46
batch 0 / 46 loss: 0.04866188392043114
Testing (cite this): batch 1 / 46
Testing (cite this): batch 2 / 46
Testing (cite this): batch 3 / 46
Testing (cite this): batch 4 / 46
Testing (cite this): batch 5 / 46
Testing (cite this): batch 6 / 46
Testing (cite this): batch 7 / 46
Testing (cite this): batch 8 / 46
Testing (cite this): batch 9 / 46
Testing (cite this): batch 10 / 46
Testing (cite this): batch 11 / 46
Testing (cite this): batch 12 / 46
Testing (cite this): batch 13 / 46
Testing (cite this): batch 14 / 46
Testing (cite this): batch 15 / 46
Testing (cite this): batch 16 / 46
Testing (cite this): batch 17 / 46
Testing (cite this): batch 18 / 46
Testing (cite this): batch 19 / 46
Testing (cite this): batch 20 / 46
Testing (cite this): batch 21 / 46
Testing (cite this): batch 22 / 46
Testing (cite this): batch 23 / 46
Testing (cite this): batch 24 / 46
Testing (cite this): batch 25 / 46
Testing (cite this): batch 26 / 46
Testing (cite this): batch 27 / 46
Testing (cite this): batch 28 / 46
Testing (cite this): batch 29 / 46
Testing (cite this): batch 30 / 46
Testing (cite this): batch 31 / 46
Testing (cite this): batch 32 / 46
Testing (cite this): batch 33 / 46
Testing (cite this): batch 34 / 46
Testing (cite this): batch 35 / 46
Testing (cite this): batch 36 / 46
Testing (cite this): batch 37 / 46
Testing (cite this): batch 38 / 46
Testing (cite this): batch 39 / 46
Testing (cite this): batch 40 / 46
Testing (cite this): batch 41 / 46
Testing (cite this): batch 42 / 46
Testing (cite this): batch 43 / 46
Testing (cite this): batch 44 / 46
Testing (cite this): batch 45 / 46
total number of ranks, torch.Size([5848])
====== Ranks ======
ranks size: torch.Size([5848])
test_loss: 3.4231459759175777
mr: 6761.62744140625
mrr: 0.4085741341114044
h1: 0.38354992866516113
h3: 0.41774964332580566
h5: 0.4351915121078491
h10: 0.4606703221797943
==================================

Done Testing!
done with training and eval
