['run_exp.py', '0', 'DBpedia50', '20', '5e-5', 'zscore', '128', '64', '30', '0', '1', 'simple', '1']
Creating a new model from scratch
loading NN
done loading NN
loading dataset
DBpedia50
X_p: torch.Size([32203, 23])
X_p: torch.Size([2095, 23])
X_p: torch.Size([123, 23])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 20
Epoch 1 -- batch 0 / 252 loss: 0.11107878386974335
Epoch 2 -- batch 0 / 252 loss: 0.09125198423862457
Epoch 3 -- batch 0 / 252 loss: 0.07240822911262512
Epoch 4 -- batch 0 / 252 loss: 0.05956604331731796
Epoch 5 -- batch 0 / 252 loss: 0.0449812188744545
Saving checkpoint at epoch 5; prefix = chkpt-ID_5720864915836263
Epoch 6 -- batch 0 / 252 loss: 0.030665980651974678
Epoch 7 -- batch 0 / 252 loss: 0.020149923861026764
Epoch 8 -- batch 0 / 252 loss: 0.01253676600754261
Epoch 9 -- batch 0 / 252 loss: 0.007481337990611792
Epoch 10 -- batch 0 / 252 loss: 0.004806411452591419
Saving checkpoint at epoch 10; prefix = chkpt-ID_5720864915836263
Epoch 11 -- batch 0 / 252 loss: 0.0029263333417475224
Epoch 12 -- batch 0 / 252 loss: 0.0018836456583812833
Epoch 13 -- batch 0 / 252 loss: 0.0013990208972245455
Epoch 14 -- batch 0 / 252 loss: 0.000820345536340028
Epoch 15 -- batch 0 / 252 loss: 0.0008179215947166085
Saving checkpoint at epoch 15; prefix = chkpt-ID_5720864915836263
Epoch 16 -- batch 0 / 252 loss: 0.0009335343493148685
Epoch 17 -- batch 0 / 252 loss: 0.00044060073560103774
Epoch 18 -- batch 0 / 252 loss: 0.0003355826484039426
Epoch 19 -- batch 0 / 252 loss: 0.0003004451864399016
Epoch 20 -- batch 0 / 252 loss: 0.0002746891987044364
Saving checkpoint at epoch 20; prefix = chkpt-ID_5720864915836263
Done Training!

==================================
Testing (cite this): dataloader for dataset DBpedia50
Testing (cite this): batch 0 / 2
Testing (cite this): batch 1 / 2
total number of ranks, torch.Size([123])
====== Ranks ======
ranks size: torch.Size([123])
test_loss: 0.14138366281986237
mr: 17071.345703125
mrr: 0.12185465544462204
h1: 0.08943089097738266
h3: 0.13821138441562653
h5: 0.16260161995887756
h10: 0.17886178195476532
==================================

Done Testing!
done with training and eval
Experiments took 287 seconds on 
