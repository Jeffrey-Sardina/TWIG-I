['run_exp.py', '0', 'FB15k237', '20', '5e-5', 'zscore', '128', '64', '500', '0', '1', 'simple', '1']
Creating a new model from scratch
loading NN
done loading NN
loading dataset
FB15k237
X_p: torch.Size([272115, 23])
X_p: torch.Size([20438, 23])
X_p: torch.Size([17526, 23])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 20
Epoch 1 -- batch 0 / 2126 loss: 0.10633241385221481
batch 500 / 2126 loss: 0.08889356255531311
batch 1000 / 2126 loss: 0.06628508120775223
batch 1500 / 2126 loss: 0.045284830033779144
batch 2000 / 2126 loss: 0.040981486439704895
Epoch 2 -- batch 0 / 2126 loss: 0.03598075360059738
batch 500 / 2126 loss: 0.010007158853113651
batch 1000 / 2126 loss: 0.009443926624953747
batch 1500 / 2126 loss: 0.005021148826926947
batch 2000 / 2126 loss: 0.005569645203649998
Epoch 3 -- batch 0 / 2126 loss: 0.010877007618546486
batch 500 / 2126 loss: 0.0021425820887088776
batch 1000 / 2126 loss: 0.0012214797316119075
batch 1500 / 2126 loss: 0.0017287881346419454
batch 2000 / 2126 loss: 0.0017371329013258219
Epoch 4 -- batch 0 / 2126 loss: 0.0028453138656914234
batch 500 / 2126 loss: 0.000817108666524291
batch 1000 / 2126 loss: 0.0007124044350348413
batch 1500 / 2126 loss: 0.0009608897962607443
batch 2000 / 2126 loss: 0.001211266964673996
Epoch 5 -- batch 0 / 2126 loss: 0.00139630027115345
batch 500 / 2126 loss: 0.00031808408675715327
batch 1000 / 2126 loss: 0.0005608680658042431
batch 1500 / 2126 loss: 0.0005274558207020164
batch 2000 / 2126 loss: 0.000753384898416698
Saving checkpoint at epoch 5; prefix = chkpt-ID_6663313754275415
Epoch 6 -- batch 0 / 2126 loss: 0.0008120014099404216
batch 500 / 2126 loss: 0.00014313480642158538
batch 1000 / 2126 loss: 0.0003844499296974391
batch 1500 / 2126 loss: 0.0003113322309218347
batch 2000 / 2126 loss: 0.000427375576691702
Epoch 7 -- batch 0 / 2126 loss: 0.0003904822515323758
batch 500 / 2126 loss: 0.00011013886978616938
batch 1000 / 2126 loss: 0.00025677066878415644
batch 1500 / 2126 loss: 0.0003020774747710675
batch 2000 / 2126 loss: 0.00027373680495657027
Epoch 8 -- batch 0 / 2126 loss: 0.00028558337362483144
batch 500 / 2126 loss: 0.00011874957999680191
batch 1000 / 2126 loss: 0.00012537342263385653
batch 1500 / 2126 loss: 0.00028613777249120176
batch 2000 / 2126 loss: 0.00023383850930258632
Epoch 9 -- batch 0 / 2126 loss: 0.0001851716951932758
batch 500 / 2126 loss: 6.347425369312987e-05
batch 1000 / 2126 loss: 5.025016071158461e-05
batch 1500 / 2126 loss: 0.00026817512116394937
batch 2000 / 2126 loss: 0.00020588526967912912
Epoch 10 -- batch 0 / 2126 loss: 0.00016850282554514706
batch 500 / 2126 loss: 5.107420656713657e-05
batch 1000 / 2126 loss: 5.023999256081879e-05
batch 1500 / 2126 loss: 0.0002631664683576673
batch 2000 / 2126 loss: 0.00011321555939503014
Saving checkpoint at epoch 10; prefix = chkpt-ID_6663313754275415
Epoch 11 -- batch 0 / 2126 loss: 0.00011227817594772205
batch 500 / 2126 loss: 5.621326272375882e-05
batch 1000 / 2126 loss: 4.78072943224106e-05
batch 1500 / 2126 loss: 0.0001501246151747182
batch 2000 / 2126 loss: 7.81338894739747e-05
Epoch 12 -- batch 0 / 2126 loss: 3.5901375667890534e-05
batch 500 / 2126 loss: 7.10865278961137e-05
batch 1000 / 2126 loss: 2.5878391170408577e-05
batch 1500 / 2126 loss: 0.00015488667122554034
batch 2000 / 2126 loss: 6.004127862979658e-05
Epoch 13 -- batch 0 / 2126 loss: 0.0001273615489481017
batch 500 / 2126 loss: 5.1645889470819384e-05
batch 1000 / 2126 loss: 2.4623990611871704e-05
batch 1500 / 2126 loss: 0.00015722103125881404
batch 2000 / 2126 loss: 7.256000390043482e-05
Epoch 14 -- batch 0 / 2126 loss: 3.924088377971202e-05
batch 500 / 2126 loss: 7.593195186927915e-05
batch 1000 / 2126 loss: 2.1222358554950915e-05
batch 1500 / 2126 loss: 0.0001628833997529
batch 2000 / 2126 loss: 6.840674177510664e-05
Epoch 15 -- batch 0 / 2126 loss: 2.8462420232244767e-05
batch 500 / 2126 loss: 2.5627741706557572e-05
batch 1000 / 2126 loss: 3.414702587178908e-05
batch 1500 / 2126 loss: 0.00011004596308339387
batch 2000 / 2126 loss: 5.366454570321366e-05
Saving checkpoint at epoch 15; prefix = chkpt-ID_6663313754275415
Epoch 16 -- batch 0 / 2126 loss: 2.8124199161538854e-05
batch 500 / 2126 loss: 2.887667142204009e-05
batch 1000 / 2126 loss: 2.3018279534881003e-05
batch 1500 / 2126 loss: 8.684850035933778e-05
batch 2000 / 2126 loss: 4.202967829769477e-05
Epoch 17 -- batch 0 / 2126 loss: 3.52645838574972e-05
batch 500 / 2126 loss: 4.0231418097391725e-05
batch 1000 / 2126 loss: 2.6452795282239094e-05
batch 1500 / 2126 loss: 7.056527101667598e-05
batch 2000 / 2126 loss: 3.6565641494235024e-05
Epoch 18 -- batch 0 / 2126 loss: 1.0289114470651839e-05
batch 500 / 2126 loss: 2.892480006266851e-05
batch 1000 / 2126 loss: 1.8235294191981666e-05
batch 1500 / 2126 loss: 6.653124000877142e-05
batch 2000 / 2126 loss: 2.7360350941307843e-05
Epoch 19 -- batch 0 / 2126 loss: 1.4537178685714025e-05
batch 500 / 2126 loss: 2.7789345040218905e-05
batch 1000 / 2126 loss: 2.4473047233186662e-05
batch 1500 / 2126 loss: 7.421650661854073e-05
batch 2000 / 2126 loss: 2.2036861992091872e-05
Epoch 20 -- batch 0 / 2126 loss: 5.835891897731926e-06
batch 500 / 2126 loss: 5.2552291890606284e-05
batch 1000 / 2126 loss: 1.9174587578163482e-05
