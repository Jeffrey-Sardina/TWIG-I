Running job! The arguemnts recieved are:
	 dataset_names: DBpedia50
	 model: base
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam
	 optimizer_args: {'lr': 0.005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 20, 'npp': 30, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: DBpedia50-pretrain

loading dataset
DBpedia50
X_p: torch.Size([32203, 22])
n_local: 21
X_p: torch.Size([2095, 22])
n_local: 21
X_p: torch.Size([123, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading NN
done loading NN
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 20
Epoch 1 -- batch 0 / 252 loss: 0.10489917546510696
Epoch 2 -- batch 0 / 252 loss: 0.010325003415346146
Epoch 3 -- batch 0 / 252 loss: 0.007025344297289848
Epoch 4 -- batch 0 / 252 loss: 0.006414138711988926
Epoch 5 -- batch 0 / 252 loss: 0.005100094713270664
Saving checkpoint at epoch 5; prefix = DBpedia50-pretrain-chkpt-ID_2429873403687784
Epoch 6 -- batch 0 / 252 loss: 0.00428304448723793
Epoch 7 -- batch 0 / 252 loss: 0.004049137234687805
Epoch 8 -- batch 0 / 252 loss: 0.004259636625647545
Epoch 9 -- batch 0 / 252 loss: 0.002786350902169943
Epoch 10 -- batch 0 / 252 loss: 0.0017494929488748312
Saving checkpoint at epoch 10; prefix = DBpedia50-pretrain-chkpt-ID_2429873403687784
Epoch 11 -- batch 0 / 252 loss: 0.0018408714095130563
Epoch 12 -- batch 0 / 252 loss: 0.002962458413094282
Epoch 13 -- batch 0 / 252 loss: 0.0017563741421326995
Epoch 14 -- batch 0 / 252 loss: 0.0023248796351253986
Epoch 15 -- batch 0 / 252 loss: 0.0009618274052627385
Saving checkpoint at epoch 15; prefix = DBpedia50-pretrain-chkpt-ID_2429873403687784
Epoch 16 -- batch 0 / 252 loss: 0.0011166371405124664
Epoch 17 -- batch 0 / 252 loss: 0.0014884450938552618
Epoch 18 -- batch 0 / 252 loss: 0.0011500295950099826
Epoch 19 -- batch 0 / 252 loss: 0.0018050586804747581
Epoch 20 -- batch 0 / 252 loss: 0.0017522255657240748
Saving checkpoint at epoch 20; prefix = DBpedia50-pretrain-chkpt-ID_2429873403687784
Done Training!

==================================
Testing (cite this): dataloader for dataset DBpedia50
Testing (cite this): batch 0 / 33
batch 0 / 33 loss: 0.08477557450532913
Testing (cite this): batch 1 / 33
Testing (cite this): batch 2 / 33
Testing (cite this): batch 3 / 33
Testing (cite this): batch 4 / 33
Testing (cite this): batch 5 / 33
Testing (cite this): batch 6 / 33
Testing (cite this): batch 7 / 33
Testing (cite this): batch 8 / 33
Testing (cite this): batch 9 / 33
Testing (cite this): batch 10 / 33
Testing (cite this): batch 11 / 33
Testing (cite this): batch 12 / 33
Testing (cite this): batch 13 / 33
Testing (cite this): batch 14 / 33
Testing (cite this): batch 15 / 33
Testing (cite this): batch 16 / 33
Testing (cite this): batch 17 / 33
Testing (cite this): batch 18 / 33
Testing (cite this): batch 19 / 33
Testing (cite this): batch 20 / 33
Testing (cite this): batch 21 / 33
Testing (cite this): batch 22 / 33
Testing (cite this): batch 23 / 33
Testing (cite this): batch 24 / 33
Testing (cite this): batch 25 / 33
Testing (cite this): batch 26 / 33
Testing (cite this): batch 27 / 33
Testing (cite this): batch 28 / 33
Testing (cite this): batch 29 / 33
Testing (cite this): batch 30 / 33
Testing (cite this): batch 31 / 33
Testing (cite this): batch 32 / 33
total number of ranks, torch.Size([4190])
====== Ranks ======
ranks size: torch.Size([4190])
test_loss: 3.06756404787302
mr: 7160.65478515625
mrr: 0.3032892346382141
h1: 0.28568020462989807
h3: 0.3109785318374634
h5: 0.31933173537254333
h10: 0.33412888646125793
==================================

Done Testing!
done with training and eval
Running job! The arguemnts recieved are:
	 dataset_names: WN18RR
	 model: base
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam
	 optimizer_args: {'lr': 0.005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 20, 'npp': 500, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: WN18RR-pretrain

loading dataset
WN18RR
X_p: torch.Size([86835, 22])
n_local: 21
X_p: torch.Size([2924, 22])
n_local: 21
X_p: torch.Size([2824, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading NN
done loading NN
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 20
Epoch 1 -- batch 0 / 679 loss: 0.1037616953253746
batch 500 / 679 loss: 0.008757776580750942
Epoch 2 -- batch 0 / 679 loss: 0.05125195533037186
batch 500 / 679 loss: 0.004576500505208969
Epoch 3 -- batch 0 / 679 loss: 0.03927364945411682
batch 500 / 679 loss: 0.005256637465208769
Epoch 4 -- batch 0 / 679 loss: 0.03470465540885925
batch 500 / 679 loss: 0.0037223277613520622
Epoch 5 -- batch 0 / 679 loss: 0.029361244291067123
batch 500 / 679 loss: 0.0027938487473875284
Saving checkpoint at epoch 5; prefix = WN18RR-pretrain-chkpt-ID_5360871134295512
Epoch 6 -- batch 0 / 679 loss: 0.03011961653828621
batch 500 / 679 loss: 0.0028212289325892925
Epoch 7 -- batch 0 / 679 loss: 0.028538778424263
batch 500 / 679 loss: 0.0033264809753745794
Epoch 8 -- batch 0 / 679 loss: 0.02738899737596512
batch 500 / 679 loss: 0.004860340151935816
Epoch 9 -- batch 0 / 679 loss: 0.023111769929528236
batch 500 / 679 loss: 0.0029663387686014175
Epoch 10 -- batch 0 / 679 loss: 0.022885948419570923
batch 500 / 679 loss: 0.003036694135516882
Saving checkpoint at epoch 10; prefix = WN18RR-pretrain-chkpt-ID_5360871134295512
Epoch 11 -- batch 0 / 679 loss: 0.021906442940235138
batch 500 / 679 loss: 0.002166943158954382
Epoch 12 -- batch 0 / 679 loss: 0.018680572509765625
batch 500 / 679 loss: 0.003631104715168476
Epoch 13 -- batch 0 / 679 loss: 0.025910506024956703
batch 500 / 679 loss: 0.0021755348425358534
Epoch 14 -- batch 0 / 679 loss: 0.023613538593053818
batch 500 / 679 loss: 0.001946600852534175
Epoch 15 -- batch 0 / 679 loss: 0.020722750574350357
batch 500 / 679 loss: 0.0026991020422428846
Saving checkpoint at epoch 15; prefix = WN18RR-pretrain-chkpt-ID_5360871134295512
Epoch 16 -- batch 0 / 679 loss: 0.021222375333309174
batch 500 / 679 loss: 0.0028495485894382
Epoch 17 -- batch 0 / 679 loss: 0.0202257689088583
batch 500 / 679 loss: 0.002327964175492525
Epoch 18 -- batch 0 / 679 loss: 0.026657119393348694
batch 500 / 679 loss: 0.0033420638646930456
Epoch 19 -- batch 0 / 679 loss: 0.018383778631687164
batch 500 / 679 loss: 0.0025398891884833574
Epoch 20 -- batch 0 / 679 loss: 0.017985444515943527
batch 500 / 679 loss: 0.002226495649665594
Saving checkpoint at epoch 20; prefix = WN18RR-pretrain-chkpt-ID_5360871134295512
Done Training!

==================================
Testing (cite this): dataloader for dataset WN18RR
Testing (cite this): batch 0 / 46
batch 0 / 46 loss: 0.03538324683904648
Testing (cite this): batch 1 / 46
Testing (cite this): batch 2 / 46
Testing (cite this): batch 3 / 46
Testing (cite this): batch 4 / 46
Testing (cite this): batch 5 / 46
Testing (cite this): batch 6 / 46
Testing (cite this): batch 7 / 46
Testing (cite this): batch 8 / 46
Testing (cite this): batch 9 / 46
Testing (cite this): batch 10 / 46
Testing (cite this): batch 11 / 46
Testing (cite this): batch 12 / 46
Testing (cite this): batch 13 / 46
Testing (cite this): batch 14 / 46
Testing (cite this): batch 15 / 46
Testing (cite this): batch 16 / 46
Testing (cite this): batch 17 / 46
Testing (cite this): batch 18 / 46
Testing (cite this): batch 19 / 46
Testing (cite this): batch 20 / 46
Testing (cite this): batch 21 / 46
Testing (cite this): batch 22 / 46
Testing (cite this): batch 23 / 46
Testing (cite this): batch 24 / 46
Testing (cite this): batch 25 / 46
Testing (cite this): batch 26 / 46
Testing (cite this): batch 27 / 46
Testing (cite this): batch 28 / 46
Testing (cite this): batch 29 / 46
Testing (cite this): batch 30 / 46
Testing (cite this): batch 31 / 46
Testing (cite this): batch 32 / 46
Testing (cite this): batch 33 / 46
Testing (cite this): batch 34 / 46
Testing (cite this): batch 35 / 46
Testing (cite this): batch 36 / 46
Testing (cite this): batch 37 / 46
Testing (cite this): batch 38 / 46
Testing (cite this): batch 39 / 46
Testing (cite this): batch 40 / 46
Testing (cite this): batch 41 / 46
Testing (cite this): batch 42 / 46
Testing (cite this): batch 43 / 46
Testing (cite this): batch 44 / 46
Testing (cite this): batch 45 / 46
total number of ranks, torch.Size([5848])
====== Ranks ======
ranks size: torch.Size([5848])
test_loss: 1.531237294897437
mr: 4243.7314453125
mrr: 0.4540722966194153
h1: 0.42886456847190857
h3: 0.46580028533935547
h5: 0.47930917143821716
h10: 0.4998289942741394
==================================

Done Testing!
done with training and eval
Running job! The arguemnts recieved are:
	 dataset_names: FB15k237
	 model: base
	 negative_sampler: simple
	 loss_function: margin-ranking(0.1)
	 early_stopper: None
	 optimizer: Adam
	 optimizer_args: {'lr': 0.0005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 128, 'batch_size_test': 64, 'fts_blacklist': {'s_o_cofreq'}}
	 training_args: {'epochs': 20, 'npp': 100, 'hyp_validation_mode': False, 'valid_every_n': -1}
	 tag: FB15k237-pretrain

loading dataset
FB15k237
X_p: torch.Size([272115, 22])
n_local: 21
X_p: torch.Size([20438, 22])
n_local: 21
X_p: torch.Size([17526, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading NN
done loading NN
loading negative samplers
init negative sampler with args
	fts_blacklist: {'s_o_cofreq'}
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 20
Epoch 1 -- batch 0 / 2126 loss: 0.10410027951002121
batch 500 / 2126 loss: 0.04406735301017761
batch 1000 / 2126 loss: 0.035731151700019836
batch 1500 / 2126 loss: 0.0340527705848217
batch 2000 / 2126 loss: 0.036782752722501755
Epoch 2 -- batch 0 / 2126 loss: 0.04365617036819458
batch 500 / 2126 loss: 0.03162674233317375
batch 1000 / 2126 loss: 0.02699427306652069
batch 1500 / 2126 loss: 0.018931321799755096
batch 2000 / 2126 loss: 0.03047066181898117
Epoch 3 -- batch 0 / 2126 loss: 0.029287373647093773
batch 500 / 2126 loss: 0.024864714592695236
batch 1000 / 2126 loss: 0.02230064757168293
batch 1500 / 2126 loss: 0.010439111851155758
batch 2000 / 2126 loss: 0.022176537662744522
Epoch 4 -- batch 0 / 2126 loss: 0.021263759583234787
batch 500 / 2126 loss: 0.019044168293476105
batch 1000 / 2126 loss: 0.016638634726405144
batch 1500 / 2126 loss: 0.00899695884436369
batch 2000 / 2126 loss: 0.018344763666391373
Epoch 5 -- batch 0 / 2126 loss: 0.019694238901138306
batch 500 / 2126 loss: 0.015665758401155472
batch 1000 / 2126 loss: 0.007129780016839504
batch 1500 / 2126 loss: 0.006273344159126282
batch 2000 / 2126 loss: 0.014602258801460266
Saving checkpoint at epoch 5; prefix = FB15k237-pretrain-chkpt-ID_6385341367018417
Epoch 6 -- batch 0 / 2126 loss: 0.015218482352793217
batch 500 / 2126 loss: 0.015665259212255478
batch 1000 / 2126 loss: 0.004781391471624374
batch 1500 / 2126 loss: 0.005643502343446016
batch 2000 / 2126 loss: 0.010241761803627014
Epoch 7 -- batch 0 / 2126 loss: 0.012061899527907372
batch 500 / 2126 loss: 0.010621299967169762
batch 1000 / 2126 loss: 0.005992249585688114
batch 1500 / 2126 loss: 0.005636021960526705
batch 2000 / 2126 loss: 0.008691621944308281
Epoch 8 -- batch 0 / 2126 loss: 0.010502977296710014
batch 500 / 2126 loss: 0.008611636236310005
batch 1000 / 2126 loss: 0.005444787908345461
batch 1500 / 2126 loss: 0.005166961811482906
batch 2000 / 2126 loss: 0.007341455202549696
Epoch 9 -- batch 0 / 2126 loss: 0.008648657239973545
batch 500 / 2126 loss: 0.007618591655045748
batch 1000 / 2126 loss: 0.0028520438354462385
batch 1500 / 2126 loss: 0.004247055854648352
batch 2000 / 2126 loss: 0.008966547437012196
Epoch 10 -- batch 0 / 2126 loss: 0.007416983135044575
batch 500 / 2126 loss: 0.006461570970714092
batch 1000 / 2126 loss: 0.0034322908613830805
batch 1500 / 2126 loss: 0.003761415835469961
batch 2000 / 2126 loss: 0.006951149553060532
Saving checkpoint at epoch 10; prefix = FB15k237-pretrain-chkpt-ID_6385341367018417
Epoch 11 -- batch 0 / 2126 loss: 0.006582189816981554
batch 500 / 2126 loss: 0.006373886484652758
batch 1000 / 2126 loss: 0.0028466861695051193
batch 1500 / 2126 loss: 0.003951867111027241
batch 2000 / 2126 loss: 0.0066264476627111435
Epoch 12 -- batch 0 / 2126 loss: 0.005645192228257656
batch 500 / 2126 loss: 0.005829226225614548
batch 1000 / 2126 loss: 0.0028175730258226395
batch 1500 / 2126 loss: 0.0038380667101591825
batch 2000 / 2126 loss: 0.006250470876693726
Epoch 13 -- batch 0 / 2126 loss: 0.006520005874335766
batch 500 / 2126 loss: 0.005328626837581396
batch 1000 / 2126 loss: 0.002553654834628105
batch 1500 / 2126 loss: 0.003232225775718689
batch 2000 / 2126 loss: 0.006171988323330879
Epoch 14 -- batch 0 / 2126 loss: 0.005756820552051067
batch 500 / 2126 loss: 0.005580051802098751
batch 1000 / 2126 loss: 0.0031556105241179466
batch 1500 / 2126 loss: 0.0032912169117480516
batch 2000 / 2126 loss: 0.005713484715670347
Epoch 15 -- batch 0 / 2126 loss: 0.005584798287600279
batch 500 / 2126 loss: 0.004932737909257412
batch 1000 / 2126 loss: 0.0036670470144599676
batch 1500 / 2126 loss: 0.0025920826010406017
batch 2000 / 2126 loss: 0.0059742690064013
Saving checkpoint at epoch 15; prefix = FB15k237-pretrain-chkpt-ID_6385341367018417
Epoch 16 -- batch 0 / 2126 loss: 0.005098588764667511
batch 500 / 2126 loss: 0.005376015789806843
batch 1000 / 2126 loss: 0.003448612056672573
batch 1500 / 2126 loss: 0.0034356967080384493
batch 2000 / 2126 loss: 0.006458631716668606
Epoch 17 -- batch 0 / 2126 loss: 0.004919388331472874
batch 500 / 2126 loss: 0.00474000396206975
batch 1000 / 2126 loss: 0.002888921182602644
batch 1500 / 2126 loss: 0.002367103472352028
batch 2000 / 2126 loss: 0.00422960938885808
Epoch 18 -- batch 0 / 2126 loss: 0.005268759094178677
batch 500 / 2126 loss: 0.004549644887447357
batch 1000 / 2126 loss: 0.0032120461110025644
batch 1500 / 2126 loss: 0.002516005653887987
batch 2000 / 2126 loss: 0.004365941509604454
Epoch 19 -- batch 0 / 2126 loss: 0.0038702248129993677
batch 500 / 2126 loss: 0.004889297299087048
batch 1000 / 2126 loss: 0.0032395892776548862
batch 1500 / 2126 loss: 0.0022211717441678047
batch 2000 / 2126 loss: 0.003498012199997902
Epoch 20 -- batch 0 / 2126 loss: 0.0042933812364935875
batch 500 / 2126 loss: 0.004908823408186436
batch 1000 / 2126 loss: 0.0033254632726311684
batch 1500 / 2126 loss: 0.002482467796653509
batch 2000 / 2126 loss: 0.0038698092103004456
Saving checkpoint at epoch 20; prefix = FB15k237-pretrain-chkpt-ID_6385341367018417
Done Training!

==================================
Testing (cite this): dataloader for dataset FB15k237
Testing (cite this): batch 0 / 320
batch 0 / 320 loss: 0.0015803056303411722
Testing (cite this): batch 1 / 320
Testing (cite this): batch 2 / 320
Testing (cite this): batch 3 / 320
Testing (cite this): batch 4 / 320
Testing (cite this): batch 5 / 320
Testing (cite this): batch 6 / 320
Testing (cite this): batch 7 / 320
Testing (cite this): batch 8 / 320
Testing (cite this): batch 9 / 320
Testing (cite this): batch 10 / 320
Testing (cite this): batch 11 / 320
Testing (cite this): batch 12 / 320
Testing (cite this): batch 13 / 320
Testing (cite this): batch 14 / 320
Testing (cite this): batch 15 / 320
Testing (cite this): batch 16 / 320
Testing (cite this): batch 17 / 320
Testing (cite this): batch 18 / 320
Testing (cite this): batch 19 / 320
Testing (cite this): batch 20 / 320
Testing (cite this): batch 21 / 320
Testing (cite this): batch 22 / 320
Testing (cite this): batch 23 / 320
Testing (cite this): batch 24 / 320
Testing (cite this): batch 25 / 320
Testing (cite this): batch 26 / 320
Testing (cite this): batch 27 / 320
Testing (cite this): batch 28 / 320
Testing (cite this): batch 29 / 320
Testing (cite this): batch 30 / 320
Testing (cite this): batch 31 / 320
Testing (cite this): batch 32 / 320
Testing (cite this): batch 33 / 320
Testing (cite this): batch 34 / 320
Testing (cite this): batch 35 / 320
Testing (cite this): batch 36 / 320
Testing (cite this): batch 37 / 320
Testing (cite this): batch 38 / 320
Testing (cite this): batch 39 / 320
Testing (cite this): batch 40 / 320
Testing (cite this): batch 41 / 320
Testing (cite this): batch 42 / 320
Testing (cite this): batch 43 / 320
Testing (cite this): batch 44 / 320
Testing (cite this): batch 45 / 320
Testing (cite this): batch 46 / 320
Testing (cite this): batch 47 / 320
Testing (cite this): batch 48 / 320
Testing (cite this): batch 49 / 320
Testing (cite this): batch 50 / 320
Testing (cite this): batch 51 / 320
Testing (cite this): batch 52 / 320
Testing (cite this): batch 53 / 320
Testing (cite this): batch 54 / 320
Testing (cite this): batch 55 / 320
Testing (cite this): batch 56 / 320
Testing (cite this): batch 57 / 320
Testing (cite this): batch 58 / 320
Testing (cite this): batch 59 / 320
Testing (cite this): batch 60 / 320
Testing (cite this): batch 61 / 320
Testing (cite this): batch 62 / 320
Testing (cite this): batch 63 / 320
Testing (cite this): batch 64 / 320
Testing (cite this): batch 65 / 320
Testing (cite this): batch 66 / 320
Testing (cite this): batch 67 / 320
Testing (cite this): batch 68 / 320
Testing (cite this): batch 69 / 320
Testing (cite this): batch 70 / 320
Testing (cite this): batch 71 / 320
Testing (cite this): batch 72 / 320
Testing (cite this): batch 73 / 320
Testing (cite this): batch 74 / 320
Testing (cite this): batch 75 / 320
Testing (cite this): batch 76 / 320
Testing (cite this): batch 77 / 320
Testing (cite this): batch 78 / 320
Testing (cite this): batch 79 / 320
Testing (cite this): batch 80 / 320
Testing (cite this): batch 81 / 320
Testing (cite this): batch 82 / 320
Testing (cite this): batch 83 / 320
Testing (cite this): batch 84 / 320
Testing (cite this): batch 85 / 320
Testing (cite this): batch 86 / 320
Testing (cite this): batch 87 / 320
Testing (cite this): batch 88 / 320
Testing (cite this): batch 89 / 320
Testing (cite this): batch 90 / 320
Testing (cite this): batch 91 / 320
Testing (cite this): batch 92 / 320
Testing (cite this): batch 93 / 320
Testing (cite this): batch 94 / 320
Testing (cite this): batch 95 / 320
Testing (cite this): batch 96 / 320
Testing (cite this): batch 97 / 320
Testing (cite this): batch 98 / 320
Testing (cite this): batch 99 / 320
Testing (cite this): batch 100 / 320
Testing (cite this): batch 101 / 320
Testing (cite this): batch 102 / 320
Testing (cite this): batch 103 / 320
Testing (cite this): batch 104 / 320
Testing (cite this): batch 105 / 320
Testing (cite this): batch 106 / 320
Testing (cite this): batch 107 / 320
Testing (cite this): batch 108 / 320
Testing (cite this): batch 109 / 320
Testing (cite this): batch 110 / 320
Testing (cite this): batch 111 / 320
Testing (cite this): batch 112 / 320
Testing (cite this): batch 113 / 320
Testing (cite this): batch 114 / 320
Testing (cite this): batch 115 / 320
Testing (cite this): batch 116 / 320
Testing (cite this): batch 117 / 320
Testing (cite this): batch 118 / 320
Testing (cite this): batch 119 / 320
Testing (cite this): batch 120 / 320
Testing (cite this): batch 121 / 320
Testing (cite this): batch 122 / 320
Testing (cite this): batch 123 / 320
Testing (cite this): batch 124 / 320
Testing (cite this): batch 125 / 320
Testing (cite this): batch 126 / 320
Testing (cite this): batch 127 / 320
Testing (cite this): batch 128 / 320
Testing (cite this): batch 129 / 320
Testing (cite this): batch 130 / 320
Testing (cite this): batch 131 / 320
Testing (cite this): batch 132 / 320
Testing (cite this): batch 133 / 320
Testing (cite this): batch 134 / 320
Testing (cite this): batch 135 / 320
Testing (cite this): batch 136 / 320
Testing (cite this): batch 137 / 320
Testing (cite this): batch 138 / 320
Testing (cite this): batch 139 / 320
Testing (cite this): batch 140 / 320
Testing (cite this): batch 141 / 320
Testing (cite this): batch 142 / 320
Testing (cite this): batch 143 / 320
Testing (cite this): batch 144 / 320
Testing (cite this): batch 145 / 320
Testing (cite this): batch 146 / 320
Testing (cite this): batch 147 / 320
Testing (cite this): batch 148 / 320
Testing (cite this): batch 149 / 320
Testing (cite this): batch 150 / 320
Testing (cite this): batch 151 / 320
Testing (cite this): batch 152 / 320
Testing (cite this): batch 153 / 320
Testing (cite this): batch 154 / 320
Testing (cite this): batch 155 / 320
Testing (cite this): batch 156 / 320
Testing (cite this): batch 157 / 320
Testing (cite this): batch 158 / 320
Testing (cite this): batch 159 / 320
Testing (cite this): batch 160 / 320
Testing (cite this): batch 161 / 320
Testing (cite this): batch 162 / 320
Testing (cite this): batch 163 / 320
Testing (cite this): batch 164 / 320
Testing (cite this): batch 165 / 320
Testing (cite this): batch 166 / 320
Testing (cite this): batch 167 / 320
Testing (cite this): batch 168 / 320
Testing (cite this): batch 169 / 320
Testing (cite this): batch 170 / 320
Testing (cite this): batch 171 / 320
Testing (cite this): batch 172 / 320
Testing (cite this): batch 173 / 320
Testing (cite this): batch 174 / 320
Testing (cite this): batch 175 / 320
Testing (cite this): batch 176 / 320
Testing (cite this): batch 177 / 320
Testing (cite this): batch 178 / 320
Testing (cite this): batch 179 / 320
Testing (cite this): batch 180 / 320
Testing (cite this): batch 181 / 320
Testing (cite this): batch 182 / 320
Testing (cite this): batch 183 / 320
Testing (cite this): batch 184 / 320
Testing (cite this): batch 185 / 320
Testing (cite this): batch 186 / 320
Testing (cite this): batch 187 / 320
Testing (cite this): batch 188 / 320
Testing (cite this): batch 189 / 320
Testing (cite this): batch 190 / 320
Testing (cite this): batch 191 / 320
Testing (cite this): batch 192 / 320
Testing (cite this): batch 193 / 320
Testing (cite this): batch 194 / 320
Testing (cite this): batch 195 / 320
Testing (cite this): batch 196 / 320
Testing (cite this): batch 197 / 320
Testing (cite this): batch 198 / 320
Testing (cite this): batch 199 / 320
Testing (cite this): batch 200 / 320
Testing (cite this): batch 201 / 320
Testing (cite this): batch 202 / 320
Testing (cite this): batch 203 / 320
Testing (cite this): batch 204 / 320
Testing (cite this): batch 205 / 320
Testing (cite this): batch 206 / 320
Testing (cite this): batch 207 / 320
Testing (cite this): batch 208 / 320
Testing (cite this): batch 209 / 320
Testing (cite this): batch 210 / 320
Testing (cite this): batch 211 / 320
Testing (cite this): batch 212 / 320
Testing (cite this): batch 213 / 320
Testing (cite this): batch 214 / 320
Testing (cite this): batch 215 / 320
Testing (cite this): batch 216 / 320
Testing (cite this): batch 217 / 320
Testing (cite this): batch 218 / 320
Testing (cite this): batch 219 / 320
Testing (cite this): batch 220 / 320
Testing (cite this): batch 221 / 320
Testing (cite this): batch 222 / 320
Testing (cite this): batch 223 / 320
Testing (cite this): batch 224 / 320
Testing (cite this): batch 225 / 320
Testing (cite this): batch 226 / 320
Testing (cite this): batch 227 / 320
Testing (cite this): batch 228 / 320
Testing (cite this): batch 229 / 320
Testing (cite this): batch 230 / 320
Testing (cite this): batch 231 / 320
Testing (cite this): batch 232 / 320
Testing (cite this): batch 233 / 320
Testing (cite this): batch 234 / 320
Testing (cite this): batch 235 / 320
Testing (cite this): batch 236 / 320
Testing (cite this): batch 237 / 320
Testing (cite this): batch 238 / 320
Testing (cite this): batch 239 / 320
Testing (cite this): batch 240 / 320
Testing (cite this): batch 241 / 320
Testing (cite this): batch 242 / 320
Testing (cite this): batch 243 / 320
Testing (cite this): batch 244 / 320
Testing (cite this): batch 245 / 320
Testing (cite this): batch 246 / 320
Testing (cite this): batch 247 / 320
Testing (cite this): batch 248 / 320
Testing (cite this): batch 249 / 320
Testing (cite this): batch 250 / 320
Testing (cite this): batch 251 / 320
Testing (cite this): batch 252 / 320
Testing (cite this): batch 253 / 320
Testing (cite this): batch 254 / 320
Testing (cite this): batch 255 / 320
Testing (cite this): batch 256 / 320
Testing (cite this): batch 257 / 320
Testing (cite this): batch 258 / 320
Testing (cite this): batch 259 / 320
Testing (cite this): batch 260 / 320
Testing (cite this): batch 261 / 320
Testing (cite this): batch 262 / 320
Testing (cite this): batch 263 / 320
Testing (cite this): batch 264 / 320
Testing (cite this): batch 265 / 320
Testing (cite this): batch 266 / 320
Testing (cite this): batch 267 / 320
Testing (cite this): batch 268 / 320
Testing (cite this): batch 269 / 320
Testing (cite this): batch 270 / 320
Testing (cite this): batch 271 / 320
Testing (cite this): batch 272 / 320
Testing (cite this): batch 273 / 320
Testing (cite this): batch 274 / 320
Testing (cite this): batch 275 / 320
Testing (cite this): batch 276 / 320
Testing (cite this): batch 277 / 320
Testing (cite this): batch 278 / 320
Testing (cite this): batch 279 / 320
Testing (cite this): batch 280 / 320
Testing (cite this): batch 281 / 320
Testing (cite this): batch 282 / 320
Testing (cite this): batch 283 / 320
Testing (cite this): batch 284 / 320
Testing (cite this): batch 285 / 320
Testing (cite this): batch 286 / 320
Testing (cite this): batch 287 / 320
Testing (cite this): batch 288 / 320
Testing (cite this): batch 289 / 320
Testing (cite this): batch 290 / 320
Testing (cite this): batch 291 / 320
Testing (cite this): batch 292 / 320
Testing (cite this): batch 293 / 320
Testing (cite this): batch 294 / 320
Testing (cite this): batch 295 / 320
Testing (cite this): batch 296 / 320
Testing (cite this): batch 297 / 320
Testing (cite this): batch 298 / 320
Testing (cite this): batch 299 / 320
Testing (cite this): batch 300 / 320
Testing (cite this): batch 301 / 320
Testing (cite this): batch 302 / 320
Testing (cite this): batch 303 / 320
Testing (cite this): batch 304 / 320
Testing (cite this): batch 305 / 320
Testing (cite this): batch 306 / 320
Testing (cite this): batch 307 / 320
Testing (cite this): batch 308 / 320
Testing (cite this): batch 309 / 320
Testing (cite this): batch 310 / 320
Testing (cite this): batch 311 / 320
Testing (cite this): batch 312 / 320
Testing (cite this): batch 313 / 320
Testing (cite this): batch 314 / 320
Testing (cite this): batch 315 / 320
Testing (cite this): batch 316 / 320
Testing (cite this): batch 317 / 320
Testing (cite this): batch 318 / 320
Testing (cite this): batch 319 / 320
total number of ranks, torch.Size([40876])
====== Ranks ======
ranks size: torch.Size([40876])
test_loss: 1.4081118298560966
mr: 221.8489532470703
mrr: 0.8163534998893738
h1: 0.7957725524902344
h3: 0.8210441470146179
h5: 0.8381690979003906
h10: 0.8582786917686462
==================================

Done Testing!
done with training and eval
