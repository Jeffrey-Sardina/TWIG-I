loading model settings from cache: checkpoints/FB15k237-pretrain-chkpt-ID_6385341367018417.pkl
Running on a grid of size 27
Running job! The arguemnts recieved are:
	 dataset_names: ['CoDExSmall']
	 model: TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
	 negative_sampler: simple
	 loss_function: margin-ranking(0.01)
	 early_stopper: None
	 optimizer: Adam
	 optimizer_args: {'lr': 0.005}
	 data_args: {'normalisation': 'zscore', 'batch_size': 64, 'batch_size_test': 64, 'fts_blacklist': frozenset({'s_o_cofreq'})}
	 training_args: {'epochs': 10, 'npp': 30, 'hyp_validation_mode': True, 'valid_every_n': [-1]}
	 tag: abl-FB15k237-to-CoDExSmall-from-chkpt-ID_6385341367018417

loading dataset
CoDExSmall
X_p: torch.Size([32888, 22])
n_local: 21
X_p: torch.Size([1828, 22])
n_local: 21
X_p: torch.Size([1827, 22])
n_local: 21
Using a total of 21 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	fts_blacklist: frozenset({'s_o_cofreq'})
done loading negative samplers
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=21, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 10
Epoch 1 -- batch 0 / 514 loss: 0.004906859248876572
batch 500 / 514 loss: 0.0007397911977022886
Epoch 2 -- batch 0 / 514 loss: 0.0007001569611020386
batch 500 / 514 loss: 0.0011615552939474583
Epoch 3 -- batch 0 / 514 loss: 0.0009668466518633068
