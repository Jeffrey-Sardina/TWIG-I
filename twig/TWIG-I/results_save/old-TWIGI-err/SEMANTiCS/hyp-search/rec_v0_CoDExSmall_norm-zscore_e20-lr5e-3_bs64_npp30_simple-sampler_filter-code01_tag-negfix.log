['run_exp.py', '0', 'CoDExSmall', '20', '5e-3', 'zscore', '64', '64', '30', '0', '1', 'simple', '1']
Creating a new model from scratch
loading NN
done loading NN
loading dataset
CoDExSmall
X_p: torch.Size([32888, 23])
X_p: torch.Size([1828, 23])
X_p: torch.Size([1827, 23])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 20
Epoch 1 -- batch 0 / 514 loss: 0.11145396530628204
batch 500 / 514 loss: 6.28413399681449e-05
Epoch 2 -- batch 0 / 514 loss: 0.0006528841331601143
batch 500 / 514 loss: 0.00012818508548662066
Epoch 3 -- batch 0 / 514 loss: 0.00016226495790760964
batch 500 / 514 loss: 2.876174585253466e-05
Epoch 4 -- batch 0 / 514 loss: 0.00013695930829271674
batch 500 / 514 loss: 0.0
Epoch 5 -- batch 0 / 514 loss: 0.00011398620699765161
batch 500 / 514 loss: 5.0999635277548805e-05
Saving checkpoint at epoch 5; prefix = chkpt-ID_5214071457974618
Epoch 6 -- batch 0 / 514 loss: 0.00016201793914660811
batch 500 / 514 loss: 1.921601506182924e-05
Epoch 7 -- batch 0 / 514 loss: 6.19631055087666e-06
batch 500 / 514 loss: 9.0605664126997e-07
Epoch 8 -- batch 0 / 514 loss: 0.00020447636779863387
batch 500 / 514 loss: 0.0
Epoch 9 -- batch 0 / 514 loss: 7.726178409939166e-06
batch 500 / 514 loss: 0.0
Epoch 10 -- batch 0 / 514 loss: 0.00016100864741019905
batch 500 / 514 loss: 0.0
Saving checkpoint at epoch 10; prefix = chkpt-ID_5214071457974618
Epoch 11 -- batch 0 / 514 loss: 8.999379497254267e-05
batch 500 / 514 loss: 0.0
Epoch 12 -- batch 0 / 514 loss: 4.582345354720019e-05
batch 500 / 514 loss: 0.0
Epoch 13 -- batch 0 / 514 loss: 9.688743375591002e-06
batch 500 / 514 loss: 5.526880340767093e-05
Epoch 14 -- batch 0 / 514 loss: 0.0
batch 500 / 514 loss: 0.0
Epoch 15 -- batch 0 / 514 loss: 0.0
batch 500 / 514 loss: 0.0
Saving checkpoint at epoch 15; prefix = chkpt-ID_5214071457974618
Epoch 16 -- batch 0 / 514 loss: 5.408740253187716e-05
batch 500 / 514 loss: 0.0
Epoch 17 -- batch 0 / 514 loss: 0.00024162001500371844
batch 500 / 514 loss: 0.0
Epoch 18 -- batch 0 / 514 loss: 0.0
batch 500 / 514 loss: 0.0
Epoch 19 -- batch 0 / 514 loss: 3.5642531202029204e-06
batch 500 / 514 loss: 0.0
Epoch 20 -- batch 0 / 514 loss: 5.332466389518231e-05
batch 500 / 514 loss: 1.277149476663908e-05
Saving checkpoint at epoch 20; prefix = chkpt-ID_5214071457974618
Done Training!

==================================
Testing (cite this): dataloader for dataset CoDExSmall
Testing (cite this): batch 0 / 29
Testing (cite this): batch 1 / 29
Testing (cite this): batch 2 / 29
Testing (cite this): batch 3 / 29
Testing (cite this): batch 4 / 29
Testing (cite this): batch 5 / 29
Testing (cite this): batch 6 / 29
Testing (cite this): batch 7 / 29
Testing (cite this): batch 8 / 29
Testing (cite this): batch 9 / 29
Testing (cite this): batch 10 / 29
Testing (cite this): batch 11 / 29
Testing (cite this): batch 12 / 29
Testing (cite this): batch 13 / 29
Testing (cite this): batch 14 / 29
Testing (cite this): batch 15 / 29
Testing (cite this): batch 16 / 29
Testing (cite this): batch 17 / 29
Testing (cite this): batch 18 / 29
Testing (cite this): batch 19 / 29
Testing (cite this): batch 20 / 29
Testing (cite this): batch 21 / 29
Testing (cite this): batch 22 / 29
Testing (cite this): batch 23 / 29
Testing (cite this): batch 24 / 29
Testing (cite this): batch 25 / 29
Testing (cite this): batch 26 / 29
Testing (cite this): batch 27 / 29
Testing (cite this): batch 28 / 29
total number of ranks, torch.Size([1827])
====== Ranks ======
ranks size: torch.Size([1827])
test_loss: 1.2088168561458588
mr: 174.2911834716797
mrr: 0.38955408334732056
h1: 0.33059659600257874
h3: 0.3814997375011444
h5: 0.41488778591156006
h10: 0.5298303365707397
==================================

Done Testing!
done with training and eval
Experiments took 259 seconds on 
