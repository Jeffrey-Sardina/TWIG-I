loading model settings from cache: checkpoints/chkpt-ID_8315447818826931.pkl
overriding original values for epochs. Was 100, now is 20
overriding original values for dataset_names. Was ['FB15k237'], now is ['CoDExSmall']
overriding original values for batch_size. Was 128, now is 64
overriding original values for lr. Was 0.0005, now is 0.005
overriding original values for npp. Was 100, now is 100
overriding original values for hyp_validation_mode. Was False, now is 0
It will be trained for 20 more epochs now.
If you are loading a modle that was itself loaded from checkpoint, the number of epochs listed as already completed above will be incorrect
until I get around to fxing this, you will have to recursively go through logs to find the total number of epochs run
loadng TWIG-I model from disk at: checkpoints/chkpt-ID_8315447818826931_e100.pt
the full config being used is: {'version': 'base', 'dataset_names': ['CoDExSmall'], 'epochs': 20, 'lr': 0.005, 'normalisation': 'zscore', 'batch_size': 64, 'batch_size_test': 64, 'npp': 100, 'use_train_filter': False, 'use_valid_and_test_filters': True, 'sampler_type': 'simple', 'use_2_hop_fts': True, 'fts_blacklist': {'None'}, 'hyp_validation_mode': 0}
loading dataset
CoDExSmall
X_p: torch.Size([32888, 23])
n_local: 22
X_p: torch.Size([1828, 23])
n_local: 22
X_p: torch.Size([1827, 23])
n_local: 22
Using a total of 22 features
done loading dataset
loading filters
done loading filters
loading negative samplers
init negative sampler with args
	use_2_hop_fts: True
	fts_blacklist: {'None'}
done loading negative samplers
Running in standard evaluation mode
TWIG will be evaulaited on the test set
and will not be tested each epoch on the validation set
Using provided pre-existing model
running training and eval
TWIGI_Base(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 20
Epoch 1 -- batch 0 / 514 loss: 0.006424205377697945
batch 500 / 514 loss: 0.00020626859623007476
Epoch 2 -- batch 0 / 514 loss: 0.0003642653173301369
batch 500 / 514 loss: 0.00027159892488271
Epoch 3 -- batch 0 / 514 loss: 0.00034634582698345184
batch 500 / 514 loss: 0.00014127492613624781
Epoch 4 -- batch 0 / 514 loss: 0.0003166977840010077
batch 500 / 514 loss: 0.00014374934835359454
Epoch 5 -- batch 0 / 514 loss: 0.00048173987306654453
batch 500 / 514 loss: 0.00022087161778472364
Saving checkpoint at epoch 5; prefix = chkpt-ID_1200432795303221
Epoch 6 -- batch 0 / 514 loss: 0.0007031251443549991
batch 500 / 514 loss: 0.0001675371895544231
Epoch 7 -- batch 0 / 514 loss: 0.0003987020463682711
batch 500 / 514 loss: 0.00016213752678595483
Epoch 8 -- batch 0 / 514 loss: 0.0005707975942641497
batch 500 / 514 loss: 0.00013537690392695367
Epoch 9 -- batch 0 / 514 loss: 0.0003032680251635611
batch 500 / 514 loss: 0.0001253205700777471
Epoch 10 -- batch 0 / 514 loss: 0.00031497154850512743
batch 500 / 514 loss: 9.00532104424201e-05
Saving checkpoint at epoch 10; prefix = chkpt-ID_1200432795303221
Epoch 11 -- batch 0 / 514 loss: 0.00036066179745830595
batch 500 / 514 loss: 0.0001346983335679397
Epoch 12 -- batch 0 / 514 loss: 0.0004201181582175195
batch 500 / 514 loss: 9.109990787692368e-05
Epoch 13 -- batch 0 / 514 loss: 0.00026803274522535503
batch 500 / 514 loss: 0.00033913401421159506
Epoch 14 -- batch 0 / 514 loss: 0.000332756811985746
batch 500 / 514 loss: 0.0001273073721677065
Epoch 15 -- batch 0 / 514 loss: 0.00044803402852267027
batch 500 / 514 loss: 0.00020165515888947994
Saving checkpoint at epoch 15; prefix = chkpt-ID_1200432795303221
Epoch 16 -- batch 0 / 514 loss: 0.00019154735491611063
batch 500 / 514 loss: 0.00019839713058900088
Epoch 17 -- batch 0 / 514 loss: 0.0004637872916646302
batch 500 / 514 loss: 0.00017016017227433622
Epoch 18 -- batch 0 / 514 loss: 0.00018447621550876647
batch 500 / 514 loss: 4.3359694245737046e-05
Epoch 19 -- batch 0 / 514 loss: 0.0002682627527974546
batch 500 / 514 loss: 0.00013665817095898092
Epoch 20 -- batch 0 / 514 loss: 0.00030053360387682915
batch 500 / 514 loss: 9.310459427069873e-05
Saving checkpoint at epoch 20; prefix = chkpt-ID_1200432795303221
Done Training!

==================================
Testing (cite this): dataloader for dataset CoDExSmall
Testing (cite this): batch 0 / 29
batch 0 / 29 loss: 0.07339569926261902
Testing (cite this): batch 1 / 29
Testing (cite this): batch 2 / 29
Testing (cite this): batch 3 / 29
Testing (cite this): batch 4 / 29
Testing (cite this): batch 5 / 29
Testing (cite this): batch 6 / 29
Testing (cite this): batch 7 / 29
Testing (cite this): batch 8 / 29
Testing (cite this): batch 9 / 29
Testing (cite this): batch 10 / 29
Testing (cite this): batch 11 / 29
Testing (cite this): batch 12 / 29
Testing (cite this): batch 13 / 29
Testing (cite this): batch 14 / 29
Testing (cite this): batch 15 / 29
Testing (cite this): batch 16 / 29
Testing (cite this): batch 17 / 29
Testing (cite this): batch 18 / 29
Testing (cite this): batch 19 / 29
Testing (cite this): batch 20 / 29
Testing (cite this): batch 21 / 29
Testing (cite this): batch 22 / 29
Testing (cite this): batch 23 / 29
Testing (cite this): batch 24 / 29
Testing (cite this): batch 25 / 29
Testing (cite this): batch 26 / 29
Testing (cite this): batch 27 / 29
Testing (cite this): batch 28 / 29
total number of ranks, torch.Size([3656])
====== Ranks ======
ranks size: torch.Size([3656])
test_loss: 1.958146896213293
mr: 86.74589538574219
mrr: 0.4369148313999176
h1: 0.38867613673210144
h3: 0.418763667345047
h5: 0.4414660930633545
h10: 0.5459518432617188
==================================

Done Testing!
done with training and eval
Experiments took 926 seconds on 
