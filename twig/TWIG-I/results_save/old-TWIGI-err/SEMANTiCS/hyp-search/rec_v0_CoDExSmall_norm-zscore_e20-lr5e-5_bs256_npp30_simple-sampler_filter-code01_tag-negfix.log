['run_exp.py', '0', 'CoDExSmall', '20', '5e-5', 'zscore', '256', '64', '30', '0', '1', 'simple', '1']
Creating a new model from scratch
loading NN
done loading NN
loading dataset
CoDExSmall
X_p: torch.Size([32888, 23])
X_p: torch.Size([1828, 23])
X_p: torch.Size([1827, 23])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 20
Epoch 1 -- batch 0 / 129 loss: 0.11162403970956802
Epoch 2 -- batch 0 / 129 loss: 0.10285063087940216
Epoch 3 -- batch 0 / 129 loss: 0.09259286522865295
Epoch 4 -- batch 0 / 129 loss: 0.08066615462303162
Epoch 5 -- batch 0 / 129 loss: 0.06600490212440491
Saving checkpoint at epoch 5; prefix = chkpt-ID_5695929272552838
Epoch 6 -- batch 0 / 129 loss: 0.051097217947244644
Epoch 7 -- batch 0 / 129 loss: 0.0379876010119915
Epoch 8 -- batch 0 / 129 loss: 0.02756236493587494
Epoch 9 -- batch 0 / 129 loss: 0.02110408805310726
Epoch 10 -- batch 0 / 129 loss: 0.0149614242836833
Saving checkpoint at epoch 10; prefix = chkpt-ID_5695929272552838
Epoch 11 -- batch 0 / 129 loss: 0.012589022517204285
Epoch 12 -- batch 0 / 129 loss: 0.009861684404313564
Epoch 13 -- batch 0 / 129 loss: 0.008694278076291084
Epoch 14 -- batch 0 / 129 loss: 0.007828627713024616
Epoch 15 -- batch 0 / 129 loss: 0.006749787367880344
Saving checkpoint at epoch 15; prefix = chkpt-ID_5695929272552838
Epoch 16 -- batch 0 / 129 loss: 0.006528228055685759
Epoch 17 -- batch 0 / 129 loss: 0.005946760531514883
Epoch 18 -- batch 0 / 129 loss: 0.005289474036544561
Epoch 19 -- batch 0 / 129 loss: 0.00523250550031662
Epoch 20 -- batch 0 / 129 loss: 0.004704122897237539
Saving checkpoint at epoch 20; prefix = chkpt-ID_5695929272552838
Done Training!

==================================
Testing (cite this): dataloader for dataset CoDExSmall
Testing (cite this): batch 0 / 29
Testing (cite this): batch 1 / 29
Testing (cite this): batch 2 / 29
Testing (cite this): batch 3 / 29
Testing (cite this): batch 4 / 29
Testing (cite this): batch 5 / 29
Testing (cite this): batch 6 / 29
Testing (cite this): batch 7 / 29
Testing (cite this): batch 8 / 29
Testing (cite this): batch 9 / 29
Testing (cite this): batch 10 / 29
Testing (cite this): batch 11 / 29
Testing (cite this): batch 12 / 29
Testing (cite this): batch 13 / 29
Testing (cite this): batch 14 / 29
Testing (cite this): batch 15 / 29
Testing (cite this): batch 16 / 29
Testing (cite this): batch 17 / 29
Testing (cite this): batch 18 / 29
Testing (cite this): batch 19 / 29
Testing (cite this): batch 20 / 29
Testing (cite this): batch 21 / 29
Testing (cite this): batch 22 / 29
Testing (cite this): batch 23 / 29
Testing (cite this): batch 24 / 29
Testing (cite this): batch 25 / 29
Testing (cite this): batch 26 / 29
Testing (cite this): batch 27 / 29
Testing (cite this): batch 28 / 29
total number of ranks, torch.Size([1827])
====== Ranks ======
ranks size: torch.Size([1827])
test_loss: 1.333188522607088
mr: 472.58648681640625
mrr: 0.10469398647546768
h1: 0.07608100771903992
h3: 0.09359605610370636
h5: 0.10235358774662018
h10: 0.1313628852367401
==================================

Done Testing!
done with training and eval
Experiments took 249 seconds on 
