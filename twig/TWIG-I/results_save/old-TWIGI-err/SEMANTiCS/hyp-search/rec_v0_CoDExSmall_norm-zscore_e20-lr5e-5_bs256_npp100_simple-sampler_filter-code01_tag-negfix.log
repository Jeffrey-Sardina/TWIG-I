['run_exp.py', '0', 'CoDExSmall', '20', '5e-5', 'zscore', '256', '64', '100', '0', '1', 'simple', '1']
Creating a new model from scratch
loading NN
done loading NN
loading dataset
CoDExSmall
X_p: torch.Size([32888, 23])
X_p: torch.Size([1828, 23])
X_p: torch.Size([1827, 23])
done loading dataset
loading filters
done loading filters
loading negative samplers
loading triple features from cache
done loading negative samplers
Running in hyperparameter evaluation mode
TWIG will be evaulaited on the validation set
and will not be tested each epoch on the validation set
running training and eval
TWIG_KGL_v0(
  (linear_struct_1): Linear(in_features=22, out_features=10, bias=True)
  (relu_1): ReLU()
  (dropout_1): Dropout(p=0.01, inplace=False)
  (linear_struct_2): Linear(in_features=10, out_features=10, bias=True)
  (relu_2): ReLU()
  (dropout_2): Dropout(p=0.01, inplace=False)
  (linear_final): Linear(in_features=10, out_features=1, bias=True)
  (sigmoid_final): Sigmoid()
)
REC: Training with epochs = 20
Epoch 1 -- batch 0 / 129 loss: 0.11135416477918625
Epoch 2 -- batch 0 / 129 loss: 0.10267048329114914
Epoch 3 -- batch 0 / 129 loss: 0.09295348823070526
Epoch 4 -- batch 0 / 129 loss: 0.08073283731937408
Epoch 5 -- batch 0 / 129 loss: 0.06583449989557266
Saving checkpoint at epoch 5; prefix = chkpt-ID_8045845798993206
Epoch 6 -- batch 0 / 129 loss: 0.05048244446516037
Epoch 7 -- batch 0 / 129 loss: 0.03798988461494446
Epoch 8 -- batch 0 / 129 loss: 0.027868852019309998
Epoch 9 -- batch 0 / 129 loss: 0.02082912251353264
Epoch 10 -- batch 0 / 129 loss: 0.01545777264982462
Saving checkpoint at epoch 10; prefix = chkpt-ID_8045845798993206
Epoch 11 -- batch 0 / 129 loss: 0.012762563303112984
Epoch 12 -- batch 0 / 129 loss: 0.0101489732041955
Epoch 13 -- batch 0 / 129 loss: 0.008946854621171951
Epoch 14 -- batch 0 / 129 loss: 0.0076789092272520065
Epoch 15 -- batch 0 / 129 loss: 0.007090380415320396
Saving checkpoint at epoch 15; prefix = chkpt-ID_8045845798993206
Epoch 16 -- batch 0 / 129 loss: 0.006352095399051905
Epoch 17 -- batch 0 / 129 loss: 0.005922841839492321
Epoch 18 -- batch 0 / 129 loss: 0.005525772459805012
Epoch 19 -- batch 0 / 129 loss: 0.005043499171733856
Epoch 20 -- batch 0 / 129 loss: 0.005197250749915838
Saving checkpoint at epoch 20; prefix = chkpt-ID_8045845798993206
Done Training!

==================================
Testing (cite this): dataloader for dataset CoDExSmall
Testing (cite this): batch 0 / 29
Testing (cite this): batch 1 / 29
Testing (cite this): batch 2 / 29
Testing (cite this): batch 3 / 29
Testing (cite this): batch 4 / 29
Testing (cite this): batch 5 / 29
Testing (cite this): batch 6 / 29
Testing (cite this): batch 7 / 29
Testing (cite this): batch 8 / 29
Testing (cite this): batch 9 / 29
Testing (cite this): batch 10 / 29
Testing (cite this): batch 11 / 29
Testing (cite this): batch 12 / 29
Testing (cite this): batch 13 / 29
Testing (cite this): batch 14 / 29
Testing (cite this): batch 15 / 29
Testing (cite this): batch 16 / 29
Testing (cite this): batch 17 / 29
Testing (cite this): batch 18 / 29
Testing (cite this): batch 19 / 29
Testing (cite this): batch 20 / 29
Testing (cite this): batch 21 / 29
Testing (cite this): batch 22 / 29
Testing (cite this): batch 23 / 29
Testing (cite this): batch 24 / 29
Testing (cite this): batch 25 / 29
Testing (cite this): batch 26 / 29
Testing (cite this): batch 27 / 29
Testing (cite this): batch 28 / 29
total number of ranks, torch.Size([1827])
====== Ranks ======
ranks size: torch.Size([1827])
test_loss: 1.334827233105898
mr: 473.255615234375
mrr: 0.10516705363988876
h1: 0.07662834972143173
h3: 0.09195402264595032
h5: 0.10071154683828354
h10: 0.13245758414268494
==================================

Done Testing!
done with training and eval
Experiments took 364 seconds on 
